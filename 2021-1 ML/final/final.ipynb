{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38eda378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset,ConcatDataset\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Configure Random seeds\n",
    "torch.manual_seed(777)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(777)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22add21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset - train set\n",
    "class trainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "\n",
    "        self.time_train_x = torch.load('time_train_X.pt',map_location=torch.device('cpu'))\n",
    "        self.static_train_x = torch.load('static_train_X.pt',map_location=torch.device('cpu'))\n",
    "        \n",
    "        self.train_y = torch.load('train_y.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "        self.len= len(self.time_train_x)\n",
    " \n",
    "    def __getitem__(self,index):\n",
    "        return self.time_train_x[index], self.static_train_x[index],  self.train_y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Define custom dataset - test set    \n",
    "class testDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "\n",
    "        self.time_test_x = torch.load('time_test_X.pt',map_location=torch.device('cpu'))\n",
    "        self.static_test_x = torch.load('static_test_X.pt',map_location=torch.device('cpu'))\n",
    "        \n",
    "        self.test_y = torch.load('test_y.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "        self.len= len(self.time_test_x)\n",
    " \n",
    "    def __getitem__(self,index):\n",
    "        return self.time_test_x[index], self.static_test_x,  self.test_y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "# Define custom dataset - test set    \n",
    "class valDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "\n",
    "        self.time_val_x = torch.load('time_val_X.pt',map_location=torch.device('cpu'))\n",
    "        self.static_val_x = torch.load('static_val_X.pt',map_location=torch.device('cpu'))\n",
    "        \n",
    "        self.val_y = torch.load('val_y.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "        self.len= len(self.time_val_x)\n",
    " \n",
    "    def __getitem__(self,index):\n",
    "        return self.time_val_x[index], self.static_val_x[index],  self.val_y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Hyper parameter setting\n",
    "batch_size = 100\n",
    "\n",
    "# Create datsets and dataloader\n",
    "trainset = trainDataset()\n",
    "testset = testDataset()\n",
    "valset = valDataset()\n",
    "train_loader =DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(valset, batch_size = batch_size, shuffle = False)\n",
    "test_loader= DataLoader(testset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b350edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelFinal(nn.Module):\n",
    "    def __init__(self, input_size=35, hidden_size=10, num_layers=1, output_size=1):\n",
    "        super(modelFinal, self).__init__()\n",
    "        self.num_layers = num_layers \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) \n",
    "        self.fc1 =  nn.Linear(hidden_size, 32,bias=True) \n",
    "        self.fc2 = nn.Linear(7, 32,bias=True) \n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(32 + 32, 32,bias=True)\n",
    "        self.fc4 = nn.Linear(32,1,bias=True)\n",
    "\n",
    "    \n",
    "        \n",
    "        self.Sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    \n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "    def forward(self,x_time, x_static):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x_time.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x_time.size(0), self.hidden_size)) #internal state\n",
    "       \n",
    "        output, (hn, cn) = self.lstm(x_time, (h_0, c_0))\n",
    "        hn = hn.view(-1, self.hidden_size) \n",
    "        out_time = self.relu(hn)\n",
    "        out_time = self.fc1(out_time) \n",
    "        out_time = self.relu(out_time)\n",
    "        \n",
    "        out_static = self.fc2(x_static)\n",
    "        out_static = self.relu(out_static)\n",
    "        \n",
    "        out = torch.cat((out_time, out_static), dim=1)\n",
    "       \n",
    "        out = (self.fc3(out))\n",
    "        out = (self.relu(out))\n",
    "        out = (self.fc4(out))\n",
    "\n",
    "        y_pred = self.Sigmoid(out)\n",
    "        return y_pred\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b352d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 4601 elements\n"
     ]
    }
   ],
   "source": [
    "model = modelFinal()\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "\n",
    "params = list(model.parameters())\n",
    "print(\"The number of parameters:\", sum([p.numel() for p in model.parameters() if p.requires_grad]), \"elements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3723037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Training Loss: 0.13919957129567756    Validation Loss: 0.15206441411473712\n",
      "Decreased Validation Loss (1000000000 ==> 0.15206441411473712)   < model saved >\n",
      "Epoch: 1     Training Loss: 0.1377117544833847    Validation Loss: 0.14997093336748296\n",
      "Decreased Validation Loss (0.15206441411473712 ==> 0.14997093336748296)   < model saved >\n",
      "Epoch: 2     Training Loss: 0.13690540129351741    Validation Loss: 0.14979648510696458\n",
      "Decreased Validation Loss (0.14997093336748296 ==> 0.14979648510696458)   < model saved >\n",
      "Epoch: 3     Training Loss: 0.13725323383383742    Validation Loss: 0.1533356993535503\n",
      "Epoch: 4     Training Loss: 0.13754702576636846    Validation Loss: 0.15141493582823237\n",
      "Epoch: 5     Training Loss: 0.1377350726418276    Validation Loss: 0.14893470991586075\n",
      "Decreased Validation Loss (0.14979648510696458 ==> 0.14893470991586075)   < model saved >\n",
      "Epoch: 6     Training Loss: 0.13741017941240707    Validation Loss: 0.15146696353789235\n",
      "Epoch: 7     Training Loss: 0.13784782789884523    Validation Loss: 0.15458299140216875\n",
      "Epoch: 8     Training Loss: 0.13674521211916482    Validation Loss: 0.15309455163288313\n",
      "Epoch: 9     Training Loss: 0.13562991579356126    Validation Loss: 0.15294519982865598\n",
      "Epoch: 10     Training Loss: 0.1354895017303974    Validation Loss: 0.15391631351142634\n",
      "Epoch: 11     Training Loss: 0.13482537015272114    Validation Loss: 0.1534426002534198\n",
      "Epoch: 12     Training Loss: 0.13492320911043945    Validation Loss: 0.15213910404775963\n",
      "Epoch: 13     Training Loss: 0.1348055385632355    Validation Loss: 0.14952619117302973\n",
      "Epoch: 14     Training Loss: 0.13385637979230486    Validation Loss: 0.15422477811330654\n",
      "Epoch: 15     Training Loss: 0.1337686580451133    Validation Loss: 0.15186080312142608\n",
      "Epoch: 16     Training Loss: 0.13375080711218157    Validation Loss: 0.15162577437328512\n",
      "Epoch: 17     Training Loss: 0.13398956593007585    Validation Loss: 0.15217629862857646\n",
      "Epoch: 18     Training Loss: 0.13356755130229905    Validation Loss: 0.15331557203756005\n",
      "Epoch: 19     Training Loss: 0.13293398016333158    Validation Loss: 0.1522705491754364\n",
      "Epoch: 20     Training Loss: 0.1323496072681443    Validation Loss: 0.15190116349668775\n",
      "Epoch: 21     Training Loss: 0.13211261000576374    Validation Loss: 0.1525671519033733\n",
      "Epoch: 22     Training Loss: 0.13280931091456025    Validation Loss: 0.1585486724850584\n",
      "Epoch: 23     Training Loss: 0.13237756992897592    Validation Loss: 0.1534958977069034\n",
      "Epoch: 24     Training Loss: 0.1317183271366379    Validation Loss: 0.15480036135824\n",
      "Epoch: 25     Training Loss: 0.13149933341903738    Validation Loss: 0.15550023928040363\n",
      "Epoch: 26     Training Loss: 0.13014433509258835    Validation Loss: 0.15295960909885462\n",
      "Epoch: 27     Training Loss: 0.13092725934598975    Validation Loss: 0.1554764115908107\n",
      "Epoch: 28     Training Loss: 0.13021773353956193    Validation Loss: 0.154230890337561\n",
      "Epoch: 29     Training Loss: 0.1298735314657642    Validation Loss: 0.16020426022835443\n",
      "Epoch: 30     Training Loss: 0.13012249522279723    Validation Loss: 0.15491997584944867\n",
      "Epoch: 31     Training Loss: 0.129464026013975    Validation Loss: 0.15458526706597844\n",
      "Epoch: 32     Training Loss: 0.13008975808248713    Validation Loss: 0.1570731328159082\n",
      "Epoch: 33     Training Loss: 0.1287737606626412    Validation Loss: 0.1528915099066789\n",
      "Epoch: 34     Training Loss: 0.12893576536314436    Validation Loss: 0.15990792416402552\n",
      "Epoch: 35     Training Loss: 0.130131085084047    Validation Loss: 0.1539026137868889\n",
      "Epoch: 36     Training Loss: 0.12969659971790684    Validation Loss: 0.15587817712641153\n",
      "Epoch: 37     Training Loss: 0.12986843426162278    Validation Loss: 0.15593792635519974\n",
      "Epoch: 38     Training Loss: 0.1314701104582725    Validation Loss: 0.15658479379337342\n",
      "Epoch: 39     Training Loss: 0.12844809612042069    Validation Loss: 0.15495334150361234\n",
      "Epoch: 40     Training Loss: 0.12889103109655448    Validation Loss: 0.15378461918625674\n",
      "Epoch: 41     Training Loss: 0.1289652248666796    Validation Loss: 0.1576520696160246\n",
      "Epoch: 42     Training Loss: 0.12923960720745076    Validation Loss: 0.15606306584887816\n",
      "Epoch: 43     Training Loss: 0.12842698849395393    Validation Loss: 0.1545229532191011\n",
      "Epoch: 44     Training Loss: 0.12775783975322338    Validation Loss: 0.1564034923422532\n",
      "Epoch: 45     Training Loss: 0.12733711868313935    Validation Loss: 0.15643798895790928\n",
      "Epoch: 46     Training Loss: 0.12805508102155405    Validation Loss: 0.15746407711603602\n",
      "Epoch: 47     Training Loss: 0.12879478735947988    Validation Loss: 0.15355787267450427\n",
      "Epoch: 48     Training Loss: 0.1269568409310002    Validation Loss: 0.15653479306912813\n",
      "Epoch: 49     Training Loss: 0.1272667876046559    Validation Loss: 0.1564991350178836\n",
      "Epoch: 50     Training Loss: 0.12674258909987898    Validation Loss: 0.15926538575745997\n",
      "Epoch: 51     Training Loss: 0.12710981528429388    Validation Loss: 0.15858372392468764\n",
      "Epoch: 52     Training Loss: 0.12647109085779518    Validation Loss: 0.15607501877868762\n",
      "Epoch: 53     Training Loss: 0.12718817604830746    Validation Loss: 0.15908701962134877\n",
      "Epoch: 54     Training Loss: 0.12638318589345515    Validation Loss: 0.16043471789262334\n",
      "Epoch: 55     Training Loss: 0.1261113560478805    Validation Loss: 0.15555964001133793\n",
      "Epoch: 56     Training Loss: 0.12491754668208609    Validation Loss: 0.16274193106371848\n",
      "Epoch: 57     Training Loss: 0.12481665620481588    Validation Loss: 0.16063790542424702\n",
      "Epoch: 58     Training Loss: 0.12450727930970411    Validation Loss: 0.16105137173025336\n",
      "Epoch: 59     Training Loss: 0.12454834511970884    Validation Loss: 0.16129980688212348\n",
      "Epoch: 60     Training Loss: 0.1243677820891879    Validation Loss: 0.16015191199105294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-ab1ffe5d3e6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_static\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-946faa4ecfa5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_time, x_static)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#internal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mhn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mout_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    663\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 110\n",
    "\n",
    "# train the model\n",
    "valid_loss_min = int(1e9)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss=0.0\n",
    "    valid_loss=0.0\n",
    "    \n",
    "    # Train \n",
    "    \n",
    "    model.train()\n",
    "    # loop for ever batch set in train dataset loader\n",
    "    for i, (data_time, data_static, target) in enumerate(train_loader):\n",
    "        \n",
    "\n",
    "        # initialize optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_size),\n",
    "                        torch.zeros(1, 1, model.hidden_size))\n",
    "        \n",
    "        \n",
    "        # train\n",
    "        y_pred = model(data_time, data_static)\n",
    "        y_pred = y_pred.squeeze()\n",
    "     \n",
    "        # caculate batch loss\n",
    "        loss = loss_function(y_pred, target)\n",
    "        \n",
    "        # update\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Record train loss\n",
    "        train_loss += loss.item() \n",
    "    \n",
    "    # Validate \n",
    "    \n",
    "    model.eval()\n",
    "    # loop for ever batch set in test dataset loader\n",
    "    for i, (data_time, data_static, target) in enumerate(val_loader):\n",
    "\n",
    "        y_pred = model(data_time, data_static)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = loss_function(y_pred, target)\n",
    "        \n",
    "        # calculate validataion loss\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    valid_loss = valid_loss / len(val_loader)\n",
    "    \n",
    "    # print training/validation results\n",
    "    print('Epoch: {}     Training Loss: {}    Validation Loss: {}'.format(epoch,train_loss, valid_loss))\n",
    "    \n",
    "    # if trained model perform best vaildation loss, then save it to the checkpoint\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Decreased Validation Loss ({} ==> {})   < model saved >'.format(valid_loss_min,valid_loss))\n",
    "        \n",
    "        checkpoint = {\n",
    "            'state_dict': model.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, './bestModel.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95537c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted F1: 0.9421980250140295\n"
     ]
    }
   ],
   "source": [
    "# Load best performance model\n",
    "checkpoint = torch.load('./bestModel.pt')\n",
    "trained_model = modelFinal()\n",
    "\n",
    "\n",
    "trained_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# evaluate\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # load test dataset\n",
    "\n",
    "    time_test_x = torch.load('time_test_X.pt',map_location=torch.device('cpu'))\n",
    "    static_test_x = torch.load('static_test_X.pt',map_location=torch.device('cpu'))\n",
    "    test_y = torch.load('test_y.pt', map_location=torch.device('cpu'))\n",
    "    # evaluate using weighted f1 score.\n",
    "    y_pred = trained_model(time_test_x, static_test_x)\n",
    "    yp = y_pred.detach().numpy()\n",
    "    yp = [1.0 if x > 0.5 else 0.0 for x in yp]\n",
    "\n",
    "    print(\"weighted F1:\", f1_score(test_y, yp, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150805c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7800d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
