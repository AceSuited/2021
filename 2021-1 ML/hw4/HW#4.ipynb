{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8515f9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 3511 elements\n"
     ]
    }
   ],
   "source": [
    "#2016112083 김연웅 HW4\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset,ConcatDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Configure Random seeds\n",
    "torch.manual_seed(777)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(777)\n",
    "\n",
    "# Define DNN Model\n",
    "class MyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.l1=torch.nn.Linear(100,30,bias=True)\n",
    "        self.l2=torch.nn.Linear(30,15,bias=True)\n",
    "        self.l3 = torch.nn.Linear(15,1,bias=True)\n",
    "            \n",
    "       \n",
    "        self.Sigmoid = torch.nn.Sigmoid()\n",
    "        self.ReLU=torch.nn.ReLU()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.ReLU((self.l1(x))))\n",
    "        x = self.dropout(self.ReLU((self.l2(x))))\n",
    "        y_pred = self.Sigmoid(self.l3(x))\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "# Define custom dataset - train set\n",
    "class HW4_trainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        train_x =np.loadtxt('./HW4_trainX.csv',delimiter=',',dtype=np.float32)\n",
    "        train_y =np.loadtxt('./HW4_trainY.csv',delimiter=',',dtype=np.float32).reshape(-1,1)\n",
    "        \n",
    "        self.len= train_x.shape[0]\n",
    "        self.x_data=torch.from_numpy(train_x)\n",
    "        self.y_data=torch.from_numpy(train_y)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Define custom dataset - test set    \n",
    "class HW4_testDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        test_x =np.loadtxt('./HW4_testX.csv',delimiter=',',dtype=np.float32)\n",
    "        test_y =np.loadtxt('./HW4_testY.csv',delimiter=',',dtype=np.float32).reshape(-1,1)\n",
    "        \n",
    "        self.len= test_x.shape[0]\n",
    "        self.x_data=torch.from_numpy(test_x)\n",
    "        self.y_data=torch.from_numpy(test_y)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Create model and print number of parameters\n",
    "model = MyModel()\n",
    "params = list(model.parameters())\n",
    "print(\"The number of parameters:\", sum([p.numel() for p in model.parameters() if p.requires_grad]), \"elements\")\n",
    "\n",
    "\n",
    "# Hyper parameter setting\n",
    "batch_size = 1000\n",
    "learning_rate =0.01\n",
    "weight_decay = 0.02\n",
    "num_epoch = 500\n",
    "momentum = 0.95\n",
    "# Create Loss function and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay=weight_decay, momentum = momentum)\n",
    "\n",
    "\n",
    "# Create datsets and dataloader\n",
    "trainset = HW4_trainDataset()\n",
    "testset = HW4_testDataset()\n",
    "train_loader =DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
    "test_loader= DataLoader(testset, batch_size = batch_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5866a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Training Loss: 0.629828829318285    Validation Loss: 0.606233075261116\n",
      "Decreased Validation Loss (1000000000 ==> 0.606233075261116)   < model saved >\n",
      "Epoch: 1     Training Loss: 0.6044873967766762    Validation Loss: 0.6002116352319717\n",
      "Decreased Validation Loss (0.606233075261116 ==> 0.6002116352319717)   < model saved >\n",
      "Epoch: 2     Training Loss: 0.5979300104081631    Validation Loss: 0.5952985286712646\n",
      "Decreased Validation Loss (0.6002116352319717 ==> 0.5952985286712646)   < model saved >\n",
      "Epoch: 3     Training Loss: 0.5943762473762035    Validation Loss: 0.5913120359182358\n",
      "Decreased Validation Loss (0.5952985286712646 ==> 0.5913120359182358)   < model saved >\n",
      "Epoch: 4     Training Loss: 0.5894307643175125    Validation Loss: 0.5892904996871948\n",
      "Decreased Validation Loss (0.5913120359182358 ==> 0.5892904996871948)   < model saved >\n",
      "Epoch: 5     Training Loss: 0.5858632437884808    Validation Loss: 0.584944948554039\n",
      "Decreased Validation Loss (0.5892904996871948 ==> 0.584944948554039)   < model saved >\n",
      "Epoch: 6     Training Loss: 0.5793402567505836    Validation Loss: 0.5800073444843292\n",
      "Decreased Validation Loss (0.584944948554039 ==> 0.5800073444843292)   < model saved >\n",
      "Epoch: 7     Training Loss: 0.5748157314956188    Validation Loss: 0.5749857127666473\n",
      "Decreased Validation Loss (0.5800073444843292 ==> 0.5749857127666473)   < model saved >\n",
      "Epoch: 8     Training Loss: 0.5695989206433296    Validation Loss: 0.5693412274122238\n",
      "Decreased Validation Loss (0.5749857127666473 ==> 0.5693412274122238)   < model saved >\n",
      "Epoch: 9     Training Loss: 0.5636946745216846    Validation Loss: 0.5625707358121872\n",
      "Decreased Validation Loss (0.5693412274122238 ==> 0.5625707358121872)   < model saved >\n",
      "Epoch: 10     Training Loss: 0.557268526405096    Validation Loss: 0.5548427253961563\n",
      "Decreased Validation Loss (0.5625707358121872 ==> 0.5548427253961563)   < model saved >\n",
      "Epoch: 11     Training Loss: 0.550187312066555    Validation Loss: 0.5474292635917664\n",
      "Decreased Validation Loss (0.5548427253961563 ==> 0.5474292635917664)   < model saved >\n",
      "Epoch: 12     Training Loss: 0.5427343770861626    Validation Loss: 0.5384988635778427\n",
      "Decreased Validation Loss (0.5474292635917664 ==> 0.5384988635778427)   < model saved >\n",
      "Epoch: 13     Training Loss: 0.5348658710718155    Validation Loss: 0.5300423055887222\n",
      "Decreased Validation Loss (0.5384988635778427 ==> 0.5300423055887222)   < model saved >\n",
      "Epoch: 14     Training Loss: 0.5262879878282547    Validation Loss: 0.5222567766904831\n",
      "Decreased Validation Loss (0.5300423055887222 ==> 0.5222567766904831)   < model saved >\n",
      "Epoch: 15     Training Loss: 0.5200048089027405    Validation Loss: 0.5138679295778275\n",
      "Decreased Validation Loss (0.5222567766904831 ==> 0.5138679295778275)   < model saved >\n",
      "Epoch: 16     Training Loss: 0.51424808986485    Validation Loss: 0.5077580139040947\n",
      "Decreased Validation Loss (0.5138679295778275 ==> 0.5077580139040947)   < model saved >\n",
      "Epoch: 17     Training Loss: 0.509037159383297    Validation Loss: 0.5015037357807159\n",
      "Decreased Validation Loss (0.5077580139040947 ==> 0.5015037357807159)   < model saved >\n",
      "Epoch: 18     Training Loss: 0.5022124983370304    Validation Loss: 0.4980313777923584\n",
      "Decreased Validation Loss (0.5015037357807159 ==> 0.4980313777923584)   < model saved >\n",
      "Epoch: 19     Training Loss: 0.49686162546277046    Validation Loss: 0.4942479580640793\n",
      "Decreased Validation Loss (0.4980313777923584 ==> 0.4942479580640793)   < model saved >\n",
      "Epoch: 20     Training Loss: 0.49306575022637844    Validation Loss: 0.4895566403865814\n",
      "Decreased Validation Loss (0.4942479580640793 ==> 0.4895566403865814)   < model saved >\n",
      "Epoch: 21     Training Loss: 0.4879904929548502    Validation Loss: 0.48592809587717056\n",
      "Decreased Validation Loss (0.4895566403865814 ==> 0.48592809587717056)   < model saved >\n",
      "Epoch: 22     Training Loss: 0.48413584381341934    Validation Loss: 0.48113055527210236\n",
      "Decreased Validation Loss (0.48592809587717056 ==> 0.48113055527210236)   < model saved >\n",
      "Epoch: 23     Training Loss: 0.48023930191993713    Validation Loss: 0.47985170781612396\n",
      "Decreased Validation Loss (0.48113055527210236 ==> 0.47985170781612396)   < model saved >\n",
      "Epoch: 24     Training Loss: 0.47596491500735283    Validation Loss: 0.4757692590355873\n",
      "Decreased Validation Loss (0.47985170781612396 ==> 0.4757692590355873)   < model saved >\n",
      "Epoch: 25     Training Loss: 0.47341104224324226    Validation Loss: 0.4718604311347008\n",
      "Decreased Validation Loss (0.4757692590355873 ==> 0.4718604311347008)   < model saved >\n",
      "Epoch: 26     Training Loss: 0.4676647111773491    Validation Loss: 0.4715958386659622\n",
      "Decreased Validation Loss (0.4718604311347008 ==> 0.4715958386659622)   < model saved >\n",
      "Epoch: 27     Training Loss: 0.4633364286273718    Validation Loss: 0.46878740191459656\n",
      "Decreased Validation Loss (0.4715958386659622 ==> 0.46878740191459656)   < model saved >\n",
      "Epoch: 28     Training Loss: 0.46120125614106655    Validation Loss: 0.46586278825998306\n",
      "Decreased Validation Loss (0.46878740191459656 ==> 0.46586278825998306)   < model saved >\n",
      "Epoch: 29     Training Loss: 0.4568558596074581    Validation Loss: 0.46416572481393814\n",
      "Decreased Validation Loss (0.46586278825998306 ==> 0.46416572481393814)   < model saved >\n",
      "Epoch: 30     Training Loss: 0.45811182633042336    Validation Loss: 0.4659721180796623\n",
      "Epoch: 31     Training Loss: 0.45293786376714706    Validation Loss: 0.46115220338106155\n",
      "Decreased Validation Loss (0.46416572481393814 ==> 0.46115220338106155)   < model saved >\n",
      "Epoch: 32     Training Loss: 0.45329229161143303    Validation Loss: 0.46032319217920303\n",
      "Decreased Validation Loss (0.46115220338106155 ==> 0.46032319217920303)   < model saved >\n",
      "Epoch: 33     Training Loss: 0.45170299895107746    Validation Loss: 0.45781801640987396\n",
      "Decreased Validation Loss (0.46032319217920303 ==> 0.45781801640987396)   < model saved >\n",
      "Epoch: 34     Training Loss: 0.4537651613354683    Validation Loss: 0.4589412808418274\n",
      "Epoch: 35     Training Loss: 0.4514825604856014    Validation Loss: 0.4577522799372673\n",
      "Decreased Validation Loss (0.45781801640987396 ==> 0.4577522799372673)   < model saved >\n",
      "Epoch: 36     Training Loss: 0.44491204246878624    Validation Loss: 0.4570240154862404\n",
      "Decreased Validation Loss (0.4577522799372673 ==> 0.4570240154862404)   < model saved >\n",
      "Epoch: 37     Training Loss: 0.4442252553999424    Validation Loss: 0.45486028492450714\n",
      "Decreased Validation Loss (0.4570240154862404 ==> 0.45486028492450714)   < model saved >\n",
      "Epoch: 38     Training Loss: 0.4423498045653105    Validation Loss: 0.4524987116456032\n",
      "Decreased Validation Loss (0.45486028492450714 ==> 0.4524987116456032)   < model saved >\n",
      "Epoch: 39     Training Loss: 0.4438403360545635    Validation Loss: 0.4517584666609764\n",
      "Decreased Validation Loss (0.4524987116456032 ==> 0.4517584666609764)   < model saved >\n",
      "Epoch: 40     Training Loss: 0.446606133133173    Validation Loss: 0.45215731114149094\n",
      "Epoch: 41     Training Loss: 0.4408330153673887    Validation Loss: 0.45264293998479843\n",
      "Epoch: 42     Training Loss: 0.43919958733022213    Validation Loss: 0.45163631439208984\n",
      "Decreased Validation Loss (0.4517584666609764 ==> 0.45163631439208984)   < model saved >\n",
      "Epoch: 43     Training Loss: 0.4434151519089937    Validation Loss: 0.45235779136419296\n",
      "Epoch: 44     Training Loss: 0.44053560495376587    Validation Loss: 0.4514102414250374\n",
      "Decreased Validation Loss (0.45163631439208984 ==> 0.4514102414250374)   < model saved >\n",
      "Epoch: 45     Training Loss: 0.44066194258630276    Validation Loss: 0.4529320150613785\n",
      "Epoch: 46     Training Loss: 0.43758752197027206    Validation Loss: 0.45394862443208694\n",
      "Epoch: 47     Training Loss: 0.43873131275177    Validation Loss: 0.45009972900152206\n",
      "Decreased Validation Loss (0.4514102414250374 ==> 0.45009972900152206)   < model saved >\n",
      "Epoch: 48     Training Loss: 0.4374150298535824    Validation Loss: 0.45165643841028214\n",
      "Epoch: 49     Training Loss: 0.43559399247169495    Validation Loss: 0.44999920576810837\n",
      "Decreased Validation Loss (0.45009972900152206 ==> 0.44999920576810837)   < model saved >\n",
      "Epoch: 50     Training Loss: 0.4346301630139351    Validation Loss: 0.4517970383167267\n",
      "Epoch: 51     Training Loss: 0.43638684414327145    Validation Loss: 0.44980181008577347\n",
      "Decreased Validation Loss (0.44999920576810837 ==> 0.44980181008577347)   < model saved >\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52     Training Loss: 0.43869162537157536    Validation Loss: 0.4503941610455513\n",
      "Epoch: 53     Training Loss: 0.4385774936527014    Validation Loss: 0.44969670474529266\n",
      "Decreased Validation Loss (0.44980181008577347 ==> 0.44969670474529266)   < model saved >\n",
      "Epoch: 54     Training Loss: 0.4369567036628723    Validation Loss: 0.4482212960720062\n",
      "Decreased Validation Loss (0.44969670474529266 ==> 0.4482212960720062)   < model saved >\n",
      "Epoch: 55     Training Loss: 0.433780325576663    Validation Loss: 0.4476252421736717\n",
      "Decreased Validation Loss (0.4482212960720062 ==> 0.4476252421736717)   < model saved >\n",
      "Epoch: 56     Training Loss: 0.4334953036159277    Validation Loss: 0.4484350234270096\n",
      "Epoch: 57     Training Loss: 0.4357123300433159    Validation Loss: 0.4465237110853195\n",
      "Decreased Validation Loss (0.4476252421736717 ==> 0.4465237110853195)   < model saved >\n",
      "Epoch: 58     Training Loss: 0.4357531238347292    Validation Loss: 0.44831616431474686\n",
      "Epoch: 59     Training Loss: 0.43584913574159145    Validation Loss: 0.44789599627256393\n",
      "Epoch: 60     Training Loss: 0.43325246684253216    Validation Loss: 0.45141807943582535\n",
      "Epoch: 61     Training Loss: 0.4345429688692093    Validation Loss: 0.4466349259018898\n",
      "Epoch: 62     Training Loss: 0.4363543130457401    Validation Loss: 0.45012887567281723\n",
      "Epoch: 63     Training Loss: 0.43398181162774563    Validation Loss: 0.44841984659433365\n",
      "Epoch: 64     Training Loss: 0.4361152518540621    Validation Loss: 0.44811706990003586\n",
      "Epoch: 65     Training Loss: 0.435071038082242    Validation Loss: 0.4481946527957916\n",
      "Epoch: 66     Training Loss: 0.4299722593277693    Validation Loss: 0.446322962641716\n",
      "Decreased Validation Loss (0.4465237110853195 ==> 0.446322962641716)   < model saved >\n",
      "Epoch: 67     Training Loss: 0.4320709630846977    Validation Loss: 0.44859927892684937\n",
      "Epoch: 68     Training Loss: 0.431266775354743    Validation Loss: 0.4467317909002304\n",
      "Epoch: 69     Training Loss: 0.4318941757082939    Validation Loss: 0.4506198763847351\n",
      "Epoch: 70     Training Loss: 0.4292943775653839    Validation Loss: 0.444904163479805\n",
      "Decreased Validation Loss (0.446322962641716 ==> 0.444904163479805)   < model saved >\n",
      "Epoch: 71     Training Loss: 0.43246345967054367    Validation Loss: 0.4461446851491928\n",
      "Epoch: 72     Training Loss: 0.43134029768407345    Validation Loss: 0.44585906714200974\n",
      "Epoch: 73     Training Loss: 0.4328709114342928    Validation Loss: 0.4478030204772949\n",
      "Epoch: 74     Training Loss: 0.4320997055619955    Validation Loss: 0.44662413746118546\n",
      "Epoch: 75     Training Loss: 0.43086480908095837    Validation Loss: 0.44581056386232376\n",
      "Epoch: 76     Training Loss: 0.43157393857836723    Validation Loss: 0.444416880607605\n",
      "Decreased Validation Loss (0.444904163479805 ==> 0.444416880607605)   < model saved >\n",
      "Epoch: 77     Training Loss: 0.4296634029597044    Validation Loss: 0.447366863489151\n",
      "Epoch: 78     Training Loss: 0.4324170704931021    Validation Loss: 0.4440757781267166\n",
      "Decreased Validation Loss (0.444416880607605 ==> 0.4440757781267166)   < model saved >\n",
      "Epoch: 79     Training Loss: 0.43024082481861115    Validation Loss: 0.447845883667469\n",
      "Epoch: 80     Training Loss: 0.4314860962331295    Validation Loss: 0.44517364352941513\n",
      "Epoch: 81     Training Loss: 0.43083543702960014    Validation Loss: 0.448890820145607\n",
      "Epoch: 82     Training Loss: 0.4332978408783674    Validation Loss: 0.4510507807135582\n",
      "Epoch: 83     Training Loss: 0.4311354234814644    Validation Loss: 0.44860417395830154\n",
      "Epoch: 84     Training Loss: 0.43045495077967644    Validation Loss: 0.4462146982550621\n",
      "Epoch: 85     Training Loss: 0.429245725274086    Validation Loss: 0.44745541363954544\n",
      "Epoch: 86     Training Loss: 0.4271922577172518    Validation Loss: 0.4441046193242073\n",
      "Epoch: 87     Training Loss: 0.4299558512866497    Validation Loss: 0.4468119665980339\n",
      "Epoch: 88     Training Loss: 0.4313561785966158    Validation Loss: 0.4467076286673546\n",
      "Epoch: 89     Training Loss: 0.43064796924591064    Validation Loss: 0.44682247936725616\n",
      "Epoch: 90     Training Loss: 0.4318485464900732    Validation Loss: 0.44574281573295593\n",
      "Epoch: 91     Training Loss: 0.434281338006258    Validation Loss: 0.44838298857212067\n",
      "Epoch: 92     Training Loss: 0.43250247091054916    Validation Loss: 0.4474063515663147\n",
      "Epoch: 93     Training Loss: 0.43384119123220444    Validation Loss: 0.4485263228416443\n",
      "Epoch: 94     Training Loss: 0.4305636268109083    Validation Loss: 0.4459269270300865\n",
      "Epoch: 95     Training Loss: 0.43043329007923603    Validation Loss: 0.4464106410741806\n",
      "Epoch: 96     Training Loss: 0.4320036470890045    Validation Loss: 0.4477577358484268\n",
      "Epoch: 97     Training Loss: 0.4323821272701025    Validation Loss: 0.447019062936306\n",
      "Epoch: 98     Training Loss: 0.4312527235597372    Validation Loss: 0.4464534819126129\n",
      "Epoch: 99     Training Loss: 0.43127371557056904    Validation Loss: 0.4466641768813133\n",
      "Epoch: 100     Training Loss: 0.42969871684908867    Validation Loss: 0.44847194105386734\n",
      "Epoch: 101     Training Loss: 0.42916106060147285    Validation Loss: 0.4476414769887924\n",
      "Epoch: 102     Training Loss: 0.4276612177491188    Validation Loss: 0.44894326478242874\n",
      "Epoch: 103     Training Loss: 0.4308527074754238    Validation Loss: 0.44590257853269577\n",
      "Epoch: 104     Training Loss: 0.43181853368878365    Validation Loss: 0.4449723958969116\n",
      "Epoch: 105     Training Loss: 0.43334180675446987    Validation Loss: 0.4468695744872093\n",
      "Epoch: 106     Training Loss: 0.43138354644179344    Validation Loss: 0.44693297892808914\n",
      "Epoch: 107     Training Loss: 0.4306889772415161    Validation Loss: 0.4443289265036583\n",
      "Epoch: 108     Training Loss: 0.4321986697614193    Validation Loss: 0.44530007988214493\n",
      "Epoch: 109     Training Loss: 0.4292913805693388    Validation Loss: 0.444302536547184\n",
      "Epoch: 110     Training Loss: 0.4319900181144476    Validation Loss: 0.4454529583454132\n",
      "Epoch: 111     Training Loss: 0.427584920078516    Validation Loss: 0.4454808905720711\n",
      "Epoch: 112     Training Loss: 0.42801101319491863    Validation Loss: 0.4479610323905945\n",
      "Epoch: 113     Training Loss: 0.43002527579665184    Validation Loss: 0.4478911831974983\n",
      "Epoch: 114     Training Loss: 0.4285922423005104    Validation Loss: 0.44565192610025406\n",
      "Epoch: 115     Training Loss: 0.429807860404253    Validation Loss: 0.44692429155111313\n",
      "Epoch: 116     Training Loss: 0.4316307604312897    Validation Loss: 0.44933638721704483\n",
      "Epoch: 117     Training Loss: 0.4293482266366482    Validation Loss: 0.4448767304420471\n",
      "Epoch: 118     Training Loss: 0.429017536342144    Validation Loss: 0.4467436820268631\n",
      "Epoch: 119     Training Loss: 0.4273231793195009    Validation Loss: 0.44663703441619873\n",
      "Epoch: 120     Training Loss: 0.4283443782478571    Validation Loss: 0.4481871575117111\n",
      "Epoch: 121     Training Loss: 0.4292854145169258    Validation Loss: 0.44327565282583237\n",
      "Decreased Validation Loss (0.4440757781267166 ==> 0.44327565282583237)   < model saved >\n",
      "Epoch: 122     Training Loss: 0.43559105321764946    Validation Loss: 0.4468611478805542\n",
      "Epoch: 123     Training Loss: 0.43200566433370113    Validation Loss: 0.44475024193525314\n",
      "Epoch: 124     Training Loss: 0.42757476307451725    Validation Loss: 0.446151927113533\n",
      "Epoch: 125     Training Loss: 0.42605042457580566    Validation Loss: 0.4445427283644676\n",
      "Epoch: 126     Training Loss: 0.4300919082015753    Validation Loss: 0.44575848430395126\n",
      "Epoch: 127     Training Loss: 0.43009329959750175    Validation Loss: 0.44733402132987976\n",
      "Epoch: 128     Training Loss: 0.4306326713413    Validation Loss: 0.4453818276524544\n",
      "Epoch: 129     Training Loss: 0.43223144300282    Validation Loss: 0.44728244096040726\n",
      "Epoch: 130     Training Loss: 0.4285538475960493    Validation Loss: 0.44751396775245667\n",
      "Epoch: 131     Training Loss: 0.4291962031275034    Validation Loss: 0.4449058771133423\n",
      "Epoch: 132     Training Loss: 0.430240023881197    Validation Loss: 0.44799474626779556\n",
      "Epoch: 133     Training Loss: 0.42957083135843277    Validation Loss: 0.4489811956882477\n",
      "Epoch: 134     Training Loss: 0.4262171480804682    Validation Loss: 0.4433657377958298\n",
      "Epoch: 135     Training Loss: 0.4273154605180025    Validation Loss: 0.44719070196151733\n",
      "Epoch: 136     Training Loss: 0.4312501437962055    Validation Loss: 0.4440719783306122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137     Training Loss: 0.431545939296484    Validation Loss: 0.44679366052150726\n",
      "Epoch: 138     Training Loss: 0.4301741402596235    Validation Loss: 0.4459208846092224\n",
      "Epoch: 139     Training Loss: 0.42860776744782925    Validation Loss: 0.44263654202222824\n",
      "Decreased Validation Loss (0.44327565282583237 ==> 0.44263654202222824)   < model saved >\n",
      "Epoch: 140     Training Loss: 0.4244915582239628    Validation Loss: 0.4451066330075264\n",
      "Epoch: 141     Training Loss: 0.42985717579722404    Validation Loss: 0.44460250437259674\n",
      "Epoch: 142     Training Loss: 0.42674767412245274    Validation Loss: 0.443964920938015\n",
      "Epoch: 143     Training Loss: 0.4269796907901764    Validation Loss: 0.4451286643743515\n",
      "Epoch: 144     Training Loss: 0.42848474346101284    Validation Loss: 0.44493769109249115\n",
      "Epoch: 145     Training Loss: 0.4262015949934721    Validation Loss: 0.4446178376674652\n",
      "Epoch: 146     Training Loss: 0.4319097213447094    Validation Loss: 0.4458685517311096\n",
      "Epoch: 147     Training Loss: 0.4283702149987221    Validation Loss: 0.4456234723329544\n",
      "Epoch: 148     Training Loss: 0.4271293096244335    Validation Loss: 0.44374699890613556\n",
      "Epoch: 149     Training Loss: 0.42573864571750164    Validation Loss: 0.4455435052514076\n",
      "Epoch: 150     Training Loss: 0.4289578180760145    Validation Loss: 0.4463585168123245\n",
      "Epoch: 151     Training Loss: 0.42967758513987064    Validation Loss: 0.44579025357961655\n",
      "Epoch: 152     Training Loss: 0.4291059710085392    Validation Loss: 0.4452323392033577\n",
      "Epoch: 153     Training Loss: 0.4312318451702595    Validation Loss: 0.4448494091629982\n",
      "Epoch: 154     Training Loss: 0.42288555204868317    Validation Loss: 0.44484589993953705\n",
      "Epoch: 155     Training Loss: 0.4271340351551771    Validation Loss: 0.4451783075928688\n",
      "Epoch: 156     Training Loss: 0.42836606688797474    Validation Loss: 0.44610951840877533\n",
      "Epoch: 157     Training Loss: 0.4297826085239649    Validation Loss: 0.4458841383457184\n",
      "Epoch: 158     Training Loss: 0.4290457386523485    Validation Loss: 0.4485727995634079\n",
      "Epoch: 159     Training Loss: 0.429368082433939    Validation Loss: 0.4488408863544464\n",
      "Epoch: 160     Training Loss: 0.42852478474378586    Validation Loss: 0.44354864209890366\n",
      "Epoch: 161     Training Loss: 0.4282704032957554    Validation Loss: 0.4487720876932144\n",
      "Epoch: 162     Training Loss: 0.43015648052096367    Validation Loss: 0.4447101652622223\n",
      "Epoch: 163     Training Loss: 0.4258368294686079    Validation Loss: 0.44612741470336914\n",
      "Epoch: 164     Training Loss: 0.42698145657777786    Validation Loss: 0.44479458779096603\n",
      "Epoch: 165     Training Loss: 0.4297560155391693    Validation Loss: 0.44321538507938385\n",
      "Epoch: 166     Training Loss: 0.4284121748059988    Validation Loss: 0.4491005018353462\n",
      "Epoch: 167     Training Loss: 0.42765611223876476    Validation Loss: 0.44626130908727646\n",
      "Epoch: 168     Training Loss: 0.4297159072011709    Validation Loss: 0.4451567679643631\n",
      "Epoch: 169     Training Loss: 0.4277190864086151    Validation Loss: 0.446363627910614\n",
      "Epoch: 170     Training Loss: 0.428092697635293    Validation Loss: 0.44372065365314484\n",
      "Epoch: 171     Training Loss: 0.4310715403407812    Validation Loss: 0.44463545083999634\n",
      "Epoch: 172     Training Loss: 0.4254763089120388    Validation Loss: 0.44781285524368286\n",
      "Epoch: 173     Training Loss: 0.43286673724651337    Validation Loss: 0.44208424538373947\n",
      "Decreased Validation Loss (0.44263654202222824 ==> 0.44208424538373947)   < model saved >\n",
      "Epoch: 174     Training Loss: 0.4272126127034426    Validation Loss: 0.4471002668142319\n",
      "Epoch: 175     Training Loss: 0.42645619064569473    Validation Loss: 0.4480079114437103\n",
      "Epoch: 176     Training Loss: 0.4280345495790243    Validation Loss: 0.4465676099061966\n",
      "Epoch: 177     Training Loss: 0.42461119033396244    Validation Loss: 0.4477872848510742\n",
      "Epoch: 178     Training Loss: 0.43188068084418774    Validation Loss: 0.44428500533103943\n",
      "Epoch: 179     Training Loss: 0.4309482127428055    Validation Loss: 0.44689086824655533\n",
      "Epoch: 180     Training Loss: 0.4268718343228102    Validation Loss: 0.44658663123846054\n",
      "Epoch: 181     Training Loss: 0.42661540023982525    Validation Loss: 0.44891949743032455\n",
      "Epoch: 182     Training Loss: 0.4283665809780359    Validation Loss: 0.4473283216357231\n",
      "Epoch: 183     Training Loss: 0.4306159261614084    Validation Loss: 0.44545404613018036\n",
      "Epoch: 184     Training Loss: 0.428839486092329    Validation Loss: 0.4457451179623604\n",
      "Epoch: 185     Training Loss: 0.4285912159830332    Validation Loss: 0.4467853680253029\n",
      "Epoch: 186     Training Loss: 0.4297325015068054    Validation Loss: 0.4473496526479721\n",
      "Epoch: 187     Training Loss: 0.4262140989303589    Validation Loss: 0.447876937687397\n",
      "Epoch: 188     Training Loss: 0.4257250167429447    Validation Loss: 0.4465251490473747\n",
      "Epoch: 189     Training Loss: 0.4271967466920614    Validation Loss: 0.4443913921713829\n",
      "Epoch: 190     Training Loss: 0.4280884601175785    Validation Loss: 0.44648975878953934\n",
      "Epoch: 191     Training Loss: 0.4299910329282284    Validation Loss: 0.4452706053853035\n",
      "Epoch: 192     Training Loss: 0.4261892959475517    Validation Loss: 0.4460885375738144\n",
      "Epoch: 193     Training Loss: 0.4273822773247957    Validation Loss: 0.4467970132827759\n",
      "Epoch: 194     Training Loss: 0.4268739428371191    Validation Loss: 0.44762352108955383\n",
      "Epoch: 195     Training Loss: 0.4279628302901983    Validation Loss: 0.4451402574777603\n",
      "Epoch: 196     Training Loss: 0.42934815026819706    Validation Loss: 0.4459184259176254\n",
      "Epoch: 197     Training Loss: 0.4262012019753456    Validation Loss: 0.445202000439167\n",
      "Epoch: 198     Training Loss: 0.42759649083018303    Validation Loss: 0.44447197020053864\n",
      "Epoch: 199     Training Loss: 0.42676446214318275    Validation Loss: 0.4425773024559021\n",
      "Epoch: 200     Training Loss: 0.4284555297344923    Validation Loss: 0.4457477852702141\n",
      "Epoch: 201     Training Loss: 0.4270969796925783    Validation Loss: 0.4462335407733917\n",
      "Epoch: 202     Training Loss: 0.42775460705161095    Validation Loss: 0.44791238009929657\n",
      "Epoch: 203     Training Loss: 0.42661491222679615    Validation Loss: 0.4484502971172333\n",
      "Epoch: 204     Training Loss: 0.42672373354434967    Validation Loss: 0.4469311386346817\n",
      "Epoch: 205     Training Loss: 0.42885593324899673    Validation Loss: 0.4453311637043953\n",
      "Epoch: 206     Training Loss: 0.4278572164475918    Validation Loss: 0.44504130631685257\n",
      "Epoch: 207     Training Loss: 0.4309564270079136    Validation Loss: 0.44813037663698196\n",
      "Epoch: 208     Training Loss: 0.43084810860455036    Validation Loss: 0.44401055574417114\n",
      "Epoch: 209     Training Loss: 0.4293479882180691    Validation Loss: 0.44539824873209\n",
      "Epoch: 210     Training Loss: 0.42873868346214294    Validation Loss: 0.4484928324818611\n",
      "Epoch: 211     Training Loss: 0.424321161583066    Validation Loss: 0.4454270824790001\n",
      "Epoch: 212     Training Loss: 0.4273386914283037    Validation Loss: 0.44491857290267944\n",
      "Epoch: 213     Training Loss: 0.425087071955204    Validation Loss: 0.4476536437869072\n",
      "Epoch: 214     Training Loss: 0.42791852727532387    Validation Loss: 0.4464888796210289\n",
      "Epoch: 215     Training Loss: 0.42939802818000317    Validation Loss: 0.44425319880247116\n",
      "Epoch: 216     Training Loss: 0.4290670957416296    Validation Loss: 0.4447220712900162\n",
      "Epoch: 217     Training Loss: 0.4274467006325722    Validation Loss: 0.44605882465839386\n",
      "Epoch: 218     Training Loss: 0.42378099635243416    Validation Loss: 0.4455209821462631\n",
      "Epoch: 219     Training Loss: 0.4291582312434912    Validation Loss: 0.4450419023633003\n",
      "Epoch: 220     Training Loss: 0.4265091437846422    Validation Loss: 0.4462369754910469\n",
      "Epoch: 221     Training Loss: 0.42624151334166527    Validation Loss: 0.44732046127319336\n",
      "Epoch: 222     Training Loss: 0.4254249595105648    Validation Loss: 0.44578080624341965\n",
      "Epoch: 223     Training Loss: 0.4295877739787102    Validation Loss: 0.44451505690813065\n",
      "Epoch: 224     Training Loss: 0.4273319598287344    Validation Loss: 0.44532760977745056\n",
      "Epoch: 225     Training Loss: 0.4308697823435068    Validation Loss: 0.4456470087170601\n",
      "Epoch: 226     Training Loss: 0.43016668781638145    Validation Loss: 0.44457632303237915\n",
      "Epoch: 227     Training Loss: 0.42815859615802765    Validation Loss: 0.4475324973464012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 228     Training Loss: 0.4291014540940523    Validation Loss: 0.44699088484048843\n",
      "Epoch: 229     Training Loss: 0.4274590443819761    Validation Loss: 0.4444108158349991\n",
      "Epoch: 230     Training Loss: 0.43112028762698174    Validation Loss: 0.4484073668718338\n",
      "Epoch: 231     Training Loss: 0.43003251776099205    Validation Loss: 0.4462350681424141\n",
      "Epoch: 232     Training Loss: 0.430971622467041    Validation Loss: 0.44624190032482147\n",
      "Epoch: 233     Training Loss: 0.4284768234938383    Validation Loss: 0.44766443222761154\n",
      "Epoch: 234     Training Loss: 0.4297017380595207    Validation Loss: 0.4466732293367386\n",
      "Epoch: 235     Training Loss: 0.43108316510915756    Validation Loss: 0.44750188291072845\n",
      "Epoch: 236     Training Loss: 0.4271904230117798    Validation Loss: 0.4457057788968086\n",
      "Epoch: 237     Training Loss: 0.4279842171818018    Validation Loss: 0.44559265673160553\n",
      "Epoch: 238     Training Loss: 0.4284876845777035    Validation Loss: 0.4444227069616318\n",
      "Epoch: 239     Training Loss: 0.4285999611020088    Validation Loss: 0.44621020555496216\n",
      "Epoch: 240     Training Loss: 0.42910700663924217    Validation Loss: 0.44687075912952423\n",
      "Epoch: 241     Training Loss: 0.4252327177673578    Validation Loss: 0.44356419891119003\n",
      "Epoch: 242     Training Loss: 0.4254226144403219    Validation Loss: 0.44355013221502304\n",
      "Epoch: 243     Training Loss: 0.42660364881157875    Validation Loss: 0.4459979608654976\n",
      "Epoch: 244     Training Loss: 0.42634036391973495    Validation Loss: 0.4476308375597\n",
      "Epoch: 245     Training Loss: 0.42639491707086563    Validation Loss: 0.44572171568870544\n",
      "Epoch: 246     Training Loss: 0.4285195879638195    Validation Loss: 0.4460395574569702\n",
      "Epoch: 247     Training Loss: 0.42715518176555634    Validation Loss: 0.4466461166739464\n",
      "Epoch: 248     Training Loss: 0.4260446634143591    Validation Loss: 0.44574519246816635\n",
      "Epoch: 249     Training Loss: 0.4291360005736351    Validation Loss: 0.4482923820614815\n",
      "Epoch: 250     Training Loss: 0.4237159714102745    Validation Loss: 0.44771658629179\n",
      "Epoch: 251     Training Loss: 0.4288069158792496    Validation Loss: 0.4493693858385086\n",
      "Epoch: 252     Training Loss: 0.4281097184866667    Validation Loss: 0.44828155636787415\n",
      "Epoch: 253     Training Loss: 0.42557390220463276    Validation Loss: 0.4459395632147789\n",
      "Epoch: 254     Training Loss: 0.42830423824489117    Validation Loss: 0.44826261699199677\n",
      "Epoch: 255     Training Loss: 0.431771581992507    Validation Loss: 0.44602087140083313\n",
      "Epoch: 256     Training Loss: 0.4250713884830475    Validation Loss: 0.4472760781645775\n",
      "Epoch: 257     Training Loss: 0.43010968528687954    Validation Loss: 0.4466632232069969\n",
      "Epoch: 258     Training Loss: 0.4300509113818407    Validation Loss: 0.4535300135612488\n",
      "Epoch: 259     Training Loss: 0.42524159513413906    Validation Loss: 0.44722968339920044\n",
      "Epoch: 260     Training Loss: 0.429295526817441    Validation Loss: 0.4513232186436653\n",
      "Epoch: 261     Training Loss: 0.4270617254078388    Validation Loss: 0.4484081268310547\n",
      "Epoch: 262     Training Loss: 0.4251008406281471    Validation Loss: 0.4493642747402191\n",
      "Epoch: 263     Training Loss: 0.4253370426595211    Validation Loss: 0.4482658579945564\n",
      "Epoch: 264     Training Loss: 0.4255288243293762    Validation Loss: 0.4450758546590805\n",
      "Epoch: 265     Training Loss: 0.4269800614565611    Validation Loss: 0.4463711231946945\n",
      "Epoch: 266     Training Loss: 0.4299164768308401    Validation Loss: 0.4470042213797569\n",
      "Epoch: 267     Training Loss: 0.427505062893033    Validation Loss: 0.44882991164922714\n",
      "Epoch: 268     Training Loss: 0.4252732675522566    Validation Loss: 0.4485305845737457\n",
      "Epoch: 269     Training Loss: 0.4290656130760908    Validation Loss: 0.44695551693439484\n",
      "Epoch: 270     Training Loss: 0.4282182976603508    Validation Loss: 0.4480695202946663\n",
      "Epoch: 271     Training Loss: 0.42830936051905155    Validation Loss: 0.4471160024404526\n",
      "Epoch: 272     Training Loss: 0.424547404050827    Validation Loss: 0.44857051223516464\n",
      "Epoch: 273     Training Loss: 0.4248902350664139    Validation Loss: 0.44794707745313644\n",
      "Epoch: 274     Training Loss: 0.4300393722951412    Validation Loss: 0.445466123521328\n",
      "Epoch: 275     Training Loss: 0.4245116375386715    Validation Loss: 0.44908256083726883\n",
      "Epoch: 276     Training Loss: 0.42587741278111935    Validation Loss: 0.4461185336112976\n",
      "Epoch: 277     Training Loss: 0.4291345328092575    Validation Loss: 0.4483368620276451\n",
      "Epoch: 278     Training Loss: 0.42925002984702587    Validation Loss: 0.4461638331413269\n",
      "Epoch: 279     Training Loss: 0.42738701589405537    Validation Loss: 0.4471810981631279\n",
      "Epoch: 280     Training Loss: 0.4241420831531286    Validation Loss: 0.44557764381170273\n",
      "Epoch: 281     Training Loss: 0.4278723504394293    Validation Loss: 0.4468754306435585\n",
      "Epoch: 282     Training Loss: 0.42904962226748466    Validation Loss: 0.4475085586309433\n",
      "Epoch: 283     Training Loss: 0.43066395074129105    Validation Loss: 0.4489769786596298\n",
      "Epoch: 284     Training Loss: 0.4244766794145107    Validation Loss: 0.4460648149251938\n",
      "Epoch: 285     Training Loss: 0.42355153523385525    Validation Loss: 0.4468391165137291\n",
      "Epoch: 286     Training Loss: 0.42583179846405983    Validation Loss: 0.44720766693353653\n",
      "Epoch: 287     Training Loss: 0.42546138539910316    Validation Loss: 0.4484173133969307\n",
      "Epoch: 288     Training Loss: 0.42777328938245773    Validation Loss: 0.44377120584249496\n",
      "Epoch: 289     Training Loss: 0.42436990700662136    Validation Loss: 0.44512178003787994\n",
      "Epoch: 290     Training Loss: 0.42944963462650776    Validation Loss: 0.4468003362417221\n",
      "Epoch: 291     Training Loss: 0.4279950950294733    Validation Loss: 0.4480060562491417\n",
      "Epoch: 292     Training Loss: 0.42460952140390873    Validation Loss: 0.4467552974820137\n",
      "Epoch: 293     Training Loss: 0.4290580116212368    Validation Loss: 0.44887711107730865\n",
      "Epoch: 294     Training Loss: 0.4277436025440693    Validation Loss: 0.44811005890369415\n",
      "Epoch: 295     Training Loss: 0.4265685863792896    Validation Loss: 0.4481828436255455\n",
      "Epoch: 296     Training Loss: 0.4237479716539383    Validation Loss: 0.44623104482889175\n",
      "Epoch: 297     Training Loss: 0.4248497374355793    Validation Loss: 0.44660529494285583\n",
      "Epoch: 298     Training Loss: 0.42996494844555855    Validation Loss: 0.4455603137612343\n",
      "Epoch: 299     Training Loss: 0.42322358675301075    Validation Loss: 0.4471483901143074\n",
      "Epoch: 300     Training Loss: 0.4293833263218403    Validation Loss: 0.44627656787633896\n",
      "Epoch: 301     Training Loss: 0.42464874498546124    Validation Loss: 0.4478294849395752\n",
      "Epoch: 302     Training Loss: 0.4298305232077837    Validation Loss: 0.446491502225399\n",
      "Epoch: 303     Training Loss: 0.4311306606978178    Validation Loss: 0.4486459866166115\n",
      "Epoch: 304     Training Loss: 0.4281449690461159    Validation Loss: 0.4480725824832916\n",
      "Epoch: 305     Training Loss: 0.4255742449313402    Validation Loss: 0.44537685811519623\n",
      "Epoch: 306     Training Loss: 0.4295215420424938    Validation Loss: 0.4475112706422806\n",
      "Epoch: 307     Training Loss: 0.42710610665380955    Validation Loss: 0.4491768851876259\n",
      "Epoch: 308     Training Loss: 0.428828464820981    Validation Loss: 0.4488051161170006\n",
      "Epoch: 309     Training Loss: 0.42756943590939045    Validation Loss: 0.4449504241347313\n",
      "Epoch: 310     Training Loss: 0.426533006131649    Validation Loss: 0.4476986303925514\n",
      "Epoch: 311     Training Loss: 0.42099424824118614    Validation Loss: 0.44708147644996643\n",
      "Epoch: 312     Training Loss: 0.4296241533011198    Validation Loss: 0.44723089039325714\n",
      "Epoch: 313     Training Loss: 0.4247883837670088    Validation Loss: 0.44799865782260895\n",
      "Epoch: 314     Training Loss: 0.42918939143419266    Validation Loss: 0.4463406652212143\n",
      "Epoch: 315     Training Loss: 0.4266718216240406    Validation Loss: 0.4485338032245636\n",
      "Epoch: 316     Training Loss: 0.4275418482720852    Validation Loss: 0.4493832290172577\n",
      "Epoch: 317     Training Loss: 0.4268946126103401    Validation Loss: 0.44591206312179565\n",
      "Epoch: 318     Training Loss: 0.42680878564715385    Validation Loss: 0.44538629800081253\n",
      "Epoch: 319     Training Loss: 0.4261352960020304    Validation Loss: 0.4485332816839218\n",
      "Epoch: 320     Training Loss: 0.4255326949059963    Validation Loss: 0.4489668533205986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 321     Training Loss: 0.42618444934487343    Validation Loss: 0.44384197145700455\n",
      "Epoch: 322     Training Loss: 0.4246606696397066    Validation Loss: 0.44440554082393646\n",
      "Epoch: 323     Training Loss: 0.4258111771196127    Validation Loss: 0.44653429090976715\n",
      "Epoch: 324     Training Loss: 0.4250571969896555    Validation Loss: 0.44648534059524536\n",
      "Epoch: 325     Training Loss: 0.42730789817869663    Validation Loss: 0.44427918642759323\n",
      "Epoch: 326     Training Loss: 0.42249152436852455    Validation Loss: 0.4457308277487755\n",
      "Epoch: 327     Training Loss: 0.4254183601588011    Validation Loss: 0.44787176698446274\n",
      "Epoch: 328     Training Loss: 0.4265903178602457    Validation Loss: 0.4462573602795601\n",
      "Epoch: 329     Training Loss: 0.4310387447476387    Validation Loss: 0.4490159600973129\n",
      "Epoch: 330     Training Loss: 0.4239142816513777    Validation Loss: 0.44601839780807495\n",
      "Epoch: 331     Training Loss: 0.42906054854393005    Validation Loss: 0.4481397792696953\n",
      "Epoch: 332     Training Loss: 0.4273961018770933    Validation Loss: 0.4478856399655342\n",
      "Epoch: 333     Training Loss: 0.42746504209935665    Validation Loss: 0.4476379528641701\n",
      "Epoch: 334     Training Loss: 0.43265062011778355    Validation Loss: 0.44634273648262024\n",
      "Epoch: 335     Training Loss: 0.42767714709043503    Validation Loss: 0.44839270412921906\n",
      "Epoch: 336     Training Loss: 0.426458265632391    Validation Loss: 0.4464264512062073\n",
      "Epoch: 337     Training Loss: 0.4295135587453842    Validation Loss: 0.44980235397815704\n",
      "Epoch: 338     Training Loss: 0.4255229365080595    Validation Loss: 0.4463201090693474\n",
      "Epoch: 339     Training Loss: 0.42367049865424633    Validation Loss: 0.4501911625266075\n",
      "Epoch: 340     Training Loss: 0.4276474956423044    Validation Loss: 0.44795434176921844\n",
      "Epoch: 341     Training Loss: 0.4251375012099743    Validation Loss: 0.4475463032722473\n",
      "Epoch: 342     Training Loss: 0.42496686801314354    Validation Loss: 0.44724565744400024\n",
      "Epoch: 343     Training Loss: 0.42429203912615776    Validation Loss: 0.44635695219039917\n",
      "Epoch: 344     Training Loss: 0.426890904083848    Validation Loss: 0.4450579807162285\n",
      "Epoch: 345     Training Loss: 0.42486825212836266    Validation Loss: 0.4463112875819206\n",
      "Epoch: 346     Training Loss: 0.42599885165691376    Validation Loss: 0.44622673839330673\n",
      "Epoch: 347     Training Loss: 0.4241561871021986    Validation Loss: 0.44885214418172836\n",
      "Epoch: 348     Training Loss: 0.4269277770072222    Validation Loss: 0.4465010464191437\n",
      "Epoch: 349     Training Loss: 0.42987934686243534    Validation Loss: 0.44953329861164093\n",
      "Epoch: 350     Training Loss: 0.42648352310061455    Validation Loss: 0.44681159406900406\n",
      "Epoch: 351     Training Loss: 0.4253514874726534    Validation Loss: 0.4505341202020645\n",
      "Epoch: 352     Training Loss: 0.4275616742670536    Validation Loss: 0.4474306032061577\n",
      "Epoch: 353     Training Loss: 0.4234270341694355    Validation Loss: 0.4477698355913162\n",
      "Epoch: 354     Training Loss: 0.42069340124726295    Validation Loss: 0.4460967928171158\n",
      "Epoch: 355     Training Loss: 0.42769428715109825    Validation Loss: 0.44664523750543594\n",
      "Epoch: 356     Training Loss: 0.42721298336982727    Validation Loss: 0.4460260719060898\n",
      "Epoch: 357     Training Loss: 0.42698851227760315    Validation Loss: 0.44738662987947464\n",
      "Epoch: 358     Training Loss: 0.42742534540593624    Validation Loss: 0.44601840525865555\n",
      "Epoch: 359     Training Loss: 0.426340539008379    Validation Loss: 0.444480299949646\n",
      "Epoch: 360     Training Loss: 0.42568795941770077    Validation Loss: 0.44672877341508865\n",
      "Epoch: 361     Training Loss: 0.4287316147238016    Validation Loss: 0.44389695674180984\n",
      "Epoch: 362     Training Loss: 0.42818176187574863    Validation Loss: 0.446081779897213\n",
      "Epoch: 363     Training Loss: 0.4281504601240158    Validation Loss: 0.4446657672524452\n",
      "Epoch: 364     Training Loss: 0.42523202672600746    Validation Loss: 0.4488537013530731\n",
      "Epoch: 365     Training Loss: 0.4268703740090132    Validation Loss: 0.44575779139995575\n",
      "Epoch: 366     Training Loss: 0.42525541223585606    Validation Loss: 0.4474715366959572\n",
      "Epoch: 367     Training Loss: 0.42614464834332466    Validation Loss: 0.44905779510736465\n",
      "Epoch: 368     Training Loss: 0.42519415356218815    Validation Loss: 0.44449884444475174\n",
      "Epoch: 369     Training Loss: 0.42180395498871803    Validation Loss: 0.4461371451616287\n",
      "Epoch: 370     Training Loss: 0.42320775985717773    Validation Loss: 0.4458143413066864\n",
      "Epoch: 371     Training Loss: 0.42599823884665966    Validation Loss: 0.4449431151151657\n",
      "Epoch: 372     Training Loss: 0.4233806524425745    Validation Loss: 0.44630206376314163\n",
      "Epoch: 373     Training Loss: 0.4254356920719147    Validation Loss: 0.44694966077804565\n",
      "Epoch: 374     Training Loss: 0.4222480095922947    Validation Loss: 0.44708026945590973\n",
      "Epoch: 375     Training Loss: 0.42202289775013924    Validation Loss: 0.4480943828821182\n",
      "Epoch: 376     Training Loss: 0.42394116148352623    Validation Loss: 0.4446898549795151\n",
      "Epoch: 377     Training Loss: 0.42614269629120827    Validation Loss: 0.44494571536779404\n",
      "Epoch: 378     Training Loss: 0.42802050337195396    Validation Loss: 0.4474177733063698\n",
      "Epoch: 379     Training Loss: 0.42397767677903175    Validation Loss: 0.4483844116330147\n",
      "Epoch: 380     Training Loss: 0.4255114998668432    Validation Loss: 0.44542598724365234\n",
      "Epoch: 381     Training Loss: 0.42727216705679893    Validation Loss: 0.4463964030146599\n",
      "Epoch: 382     Training Loss: 0.42640455439686775    Validation Loss: 0.4462478458881378\n",
      "Epoch: 383     Training Loss: 0.4231911301612854    Validation Loss: 0.4483434706926346\n",
      "Epoch: 384     Training Loss: 0.4278740882873535    Validation Loss: 0.445945680141449\n",
      "Epoch: 385     Training Loss: 0.4279203899204731    Validation Loss: 0.44782307744026184\n",
      "Epoch: 386     Training Loss: 0.42727408930659294    Validation Loss: 0.44929295778274536\n",
      "Epoch: 387     Training Loss: 0.42771785892546177    Validation Loss: 0.4509759023785591\n",
      "Epoch: 388     Training Loss: 0.42518720030784607    Validation Loss: 0.44727610796689987\n",
      "Epoch: 389     Training Loss: 0.42401540093123913    Validation Loss: 0.44730400294065475\n",
      "Epoch: 390     Training Loss: 0.42916708067059517    Validation Loss: 0.448355995118618\n",
      "Epoch: 391     Training Loss: 0.42610447108745575    Validation Loss: 0.4487665891647339\n",
      "Epoch: 392     Training Loss: 0.4275409299880266    Validation Loss: 0.44421734660863876\n",
      "Epoch: 393     Training Loss: 0.4253170844167471    Validation Loss: 0.44622986018657684\n",
      "Epoch: 394     Training Loss: 0.4282789509743452    Validation Loss: 0.44751816242933273\n",
      "Epoch: 395     Training Loss: 0.42870183661580086    Validation Loss: 0.4454767182469368\n",
      "Epoch: 396     Training Loss: 0.4253209214657545    Validation Loss: 0.44627539813518524\n",
      "Epoch: 397     Training Loss: 0.42754066176712513    Validation Loss: 0.4441201463341713\n",
      "Epoch: 398     Training Loss: 0.4279374796897173    Validation Loss: 0.44613607227802277\n",
      "Epoch: 399     Training Loss: 0.42374258674681187    Validation Loss: 0.4448339715600014\n",
      "Epoch: 400     Training Loss: 0.4242002088576555    Validation Loss: 0.44647426903247833\n",
      "Epoch: 401     Training Loss: 0.4250049777328968    Validation Loss: 0.44605059921741486\n",
      "Epoch: 402     Training Loss: 0.42334877885878086    Validation Loss: 0.44491148740053177\n",
      "Epoch: 403     Training Loss: 0.4250991027802229    Validation Loss: 0.44612111896276474\n",
      "Epoch: 404     Training Loss: 0.42508516274392605    Validation Loss: 0.44499026238918304\n",
      "Epoch: 405     Training Loss: 0.42623045295476913    Validation Loss: 0.4468921795487404\n",
      "Epoch: 406     Training Loss: 0.42576704919338226    Validation Loss: 0.4490087404847145\n",
      "Epoch: 407     Training Loss: 0.4265298508107662    Validation Loss: 0.4485314339399338\n",
      "Epoch: 408     Training Loss: 0.4282474610954523    Validation Loss: 0.44537168741226196\n",
      "Epoch: 409     Training Loss: 0.42960828356444836    Validation Loss: 0.4475027248263359\n",
      "Epoch: 410     Training Loss: 0.4276549033820629    Validation Loss: 0.4475714713335037\n",
      "Epoch: 411     Training Loss: 0.4234689921140671    Validation Loss: 0.4459274932742119\n",
      "Epoch: 412     Training Loss: 0.42261693999171257    Validation Loss: 0.44632695615291595\n",
      "Epoch: 413     Training Loss: 0.42684336192905903    Validation Loss: 0.4480857327580452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 414     Training Loss: 0.4253734238445759    Validation Loss: 0.44745396822690964\n",
      "Epoch: 415     Training Loss: 0.4286520015448332    Validation Loss: 0.44790808111429214\n",
      "Epoch: 416     Training Loss: 0.4265539962798357    Validation Loss: 0.446487121284008\n",
      "Epoch: 417     Training Loss: 0.4295752849429846    Validation Loss: 0.4451846480369568\n",
      "Epoch: 418     Training Loss: 0.4237330723553896    Validation Loss: 0.4477066323161125\n",
      "Epoch: 419     Training Loss: 0.4253308419138193    Validation Loss: 0.4462487921118736\n",
      "Epoch: 420     Training Loss: 0.4251353908330202    Validation Loss: 0.44654639810323715\n",
      "Epoch: 421     Training Loss: 0.42693736776709557    Validation Loss: 0.44693107157945633\n",
      "Epoch: 422     Training Loss: 0.42926207557320595    Validation Loss: 0.44848664104938507\n",
      "Epoch: 423     Training Loss: 0.4257946517318487    Validation Loss: 0.4471297487616539\n",
      "Epoch: 424     Training Loss: 0.42710419557988644    Validation Loss: 0.44690562039613724\n",
      "Epoch: 425     Training Loss: 0.42495858296751976    Validation Loss: 0.4489857256412506\n",
      "Epoch: 426     Training Loss: 0.42640572041273117    Validation Loss: 0.4482017084956169\n",
      "Epoch: 427     Training Loss: 0.4250527061522007    Validation Loss: 0.44884005188941956\n",
      "Epoch: 428     Training Loss: 0.42314680851995945    Validation Loss: 0.448857419192791\n",
      "Epoch: 429     Training Loss: 0.4266890063881874    Validation Loss: 0.4480172172188759\n",
      "Epoch: 430     Training Loss: 0.4284962732344866    Validation Loss: 0.44577939808368683\n",
      "Epoch: 431     Training Loss: 0.4287479296326637    Validation Loss: 0.44901276379823685\n",
      "Epoch: 432     Training Loss: 0.42724648863077164    Validation Loss: 0.44498203694820404\n",
      "Epoch: 433     Training Loss: 0.42176769487559795    Validation Loss: 0.4471278339624405\n",
      "Epoch: 434     Training Loss: 0.4255125354975462    Validation Loss: 0.44536568969488144\n",
      "Epoch: 435     Training Loss: 0.4260372407734394    Validation Loss: 0.44630393385887146\n",
      "Epoch: 436     Training Loss: 0.42532632499933243    Validation Loss: 0.44674406200647354\n",
      "Epoch: 437     Training Loss: 0.427293099462986    Validation Loss: 0.44875509291887283\n",
      "Epoch: 438     Training Loss: 0.4278328102082014    Validation Loss: 0.4450210630893707\n",
      "Epoch: 439     Training Loss: 0.42683918215334415    Validation Loss: 0.4461883157491684\n",
      "Epoch: 440     Training Loss: 0.423605153337121    Validation Loss: 0.4475799649953842\n",
      "Epoch: 441     Training Loss: 0.425266707316041    Validation Loss: 0.4449351131916046\n",
      "Epoch: 442     Training Loss: 0.42726498655974865    Validation Loss: 0.4438876733183861\n",
      "Epoch: 443     Training Loss: 0.42456765845417976    Validation Loss: 0.4467041790485382\n",
      "Epoch: 444     Training Loss: 0.4229574892669916    Validation Loss: 0.44785942137241364\n",
      "Epoch: 445     Training Loss: 0.4259281847625971    Validation Loss: 0.4466099292039871\n",
      "Epoch: 446     Training Loss: 0.4306574519723654    Validation Loss: 0.44420401006937027\n",
      "Epoch: 447     Training Loss: 0.42544382624328136    Validation Loss: 0.44839462637901306\n",
      "Epoch: 448     Training Loss: 0.4272108543664217    Validation Loss: 0.44517693668603897\n",
      "Epoch: 449     Training Loss: 0.4307446498423815    Validation Loss: 0.4466055855154991\n",
      "Epoch: 450     Training Loss: 0.4247378744184971    Validation Loss: 0.4461081251502037\n",
      "Epoch: 451     Training Loss: 0.42633096501231194    Validation Loss: 0.4485456570982933\n",
      "Epoch: 452     Training Loss: 0.42494761012494564    Validation Loss: 0.4467802345752716\n",
      "Epoch: 453     Training Loss: 0.42512740939855576    Validation Loss: 0.44869665801525116\n",
      "Epoch: 454     Training Loss: 0.42707899026572704    Validation Loss: 0.44576743245124817\n",
      "Epoch: 455     Training Loss: 0.4260272029787302    Validation Loss: 0.44424454122781754\n",
      "Epoch: 456     Training Loss: 0.42830054834485054    Validation Loss: 0.44757816940546036\n",
      "Epoch: 457     Training Loss: 0.4267557095736265    Validation Loss: 0.44433921575546265\n",
      "Epoch: 458     Training Loss: 0.4303969033062458    Validation Loss: 0.445131316781044\n",
      "Epoch: 459     Training Loss: 0.4266235716640949    Validation Loss: 0.4452809616923332\n",
      "Epoch: 460     Training Loss: 0.42672486416995525    Validation Loss: 0.4472710117697716\n",
      "Epoch: 461     Training Loss: 0.42581798508763313    Validation Loss: 0.4475259855389595\n",
      "Epoch: 462     Training Loss: 0.4256693944334984    Validation Loss: 0.44758152961730957\n",
      "Epoch: 463     Training Loss: 0.4302524942904711    Validation Loss: 0.4473762661218643\n",
      "Epoch: 464     Training Loss: 0.4253631979227066    Validation Loss: 0.44659043103456497\n",
      "Epoch: 465     Training Loss: 0.43071891926229    Validation Loss: 0.4469980150461197\n",
      "Epoch: 466     Training Loss: 0.42317155562341213    Validation Loss: 0.4446830078959465\n",
      "Epoch: 467     Training Loss: 0.4262200575321913    Validation Loss: 0.4436756297945976\n",
      "Epoch: 468     Training Loss: 0.4271757956594229    Validation Loss: 0.44661665707826614\n",
      "Epoch: 469     Training Loss: 0.4294303562492132    Validation Loss: 0.4471941888332367\n",
      "Epoch: 470     Training Loss: 0.4253712296485901    Validation Loss: 0.4450616016983986\n",
      "Epoch: 471     Training Loss: 0.42675270698964596    Validation Loss: 0.4459255486726761\n",
      "Epoch: 472     Training Loss: 0.42799231596291065    Validation Loss: 0.44594060629606247\n",
      "Epoch: 473     Training Loss: 0.4259773679077625    Validation Loss: 0.4466416612267494\n",
      "Epoch: 474     Training Loss: 0.4258785769343376    Validation Loss: 0.44530966877937317\n",
      "Epoch: 475     Training Loss: 0.42592746391892433    Validation Loss: 0.44767171144485474\n",
      "Epoch: 476     Training Loss: 0.4252950884401798    Validation Loss: 0.44495801627635956\n",
      "Epoch: 477     Training Loss: 0.4271097853779793    Validation Loss: 0.44570596516132355\n",
      "Epoch: 478     Training Loss: 0.42768574319779873    Validation Loss: 0.45102303475141525\n",
      "Epoch: 479     Training Loss: 0.42792643792927265    Validation Loss: 0.4477943554520607\n",
      "Epoch: 480     Training Loss: 0.4294949881732464    Validation Loss: 0.44968292862176895\n",
      "Epoch: 481     Training Loss: 0.4322116058319807    Validation Loss: 0.44988536089658737\n",
      "Epoch: 482     Training Loss: 0.42911374755203724    Validation Loss: 0.44536494463682175\n",
      "Epoch: 483     Training Loss: 0.4219593871384859    Validation Loss: 0.4478251039981842\n",
      "Epoch: 484     Training Loss: 0.4292186237871647    Validation Loss: 0.4453353136777878\n",
      "Epoch: 485     Training Loss: 0.4228853639215231    Validation Loss: 0.4465780034661293\n",
      "Epoch: 486     Training Loss: 0.4282884895801544    Validation Loss: 0.4464127644896507\n",
      "Epoch: 487     Training Loss: 0.4250852018594742    Validation Loss: 0.44624458253383636\n",
      "Epoch: 488     Training Loss: 0.42611473985016346    Validation Loss: 0.44571603089571\n",
      "Epoch: 489     Training Loss: 0.4259091876447201    Validation Loss: 0.4466992914676666\n",
      "Epoch: 490     Training Loss: 0.42655535973608494    Validation Loss: 0.4454060271382332\n",
      "Epoch: 491     Training Loss: 0.42607489600777626    Validation Loss: 0.4433245286345482\n",
      "Epoch: 492     Training Loss: 0.4288892559707165    Validation Loss: 0.4469808116555214\n",
      "Epoch: 493     Training Loss: 0.4270757604390383    Validation Loss: 0.4439583122730255\n",
      "Epoch: 494     Training Loss: 0.42496366426348686    Validation Loss: 0.44848865270614624\n",
      "Epoch: 495     Training Loss: 0.42647286504507065    Validation Loss: 0.44554492086172104\n",
      "Epoch: 496     Training Loss: 0.4268453400582075    Validation Loss: 0.44579675048589706\n",
      "Epoch: 497     Training Loss: 0.4276636503636837    Validation Loss: 0.44567134976387024\n",
      "Epoch: 498     Training Loss: 0.42807454615831375    Validation Loss: 0.4449107199907303\n",
      "Epoch: 499     Training Loss: 0.42204856127500534    Validation Loss: 0.4450279399752617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train the model\n",
    "valid_loss_min = int(1e9)\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    train_loss=0.0\n",
    "    valid_loss=0.0\n",
    "    \n",
    "    # Train \n",
    "    \n",
    "    model.train()\n",
    "    # loop for ever batch set in train dataset loader\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # initialize optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # train\n",
    "        y_pred = model(data)\n",
    "        \n",
    "        # caculate batch loss\n",
    "        loss = criterion(y_pred, target)\n",
    "        \n",
    "        # update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Record train loss\n",
    "        train_loss += loss.item() \n",
    "    \n",
    "    # Validate \n",
    "    \n",
    "    model.eval()\n",
    "    # loop for ever batch set in test dataset loader\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "     \n",
    "        y_pred = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(y_pred, target)\n",
    "        \n",
    "        # calculate validataion loss\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(test_loader)\n",
    "    \n",
    "    # print training/validation results\n",
    "    print('Epoch: {}     Training Loss: {}    Validation Loss: {}'.format(epoch,train_loss, valid_loss))\n",
    "    \n",
    "    # if trained model perform best vaildation loss, then save it to the checkpoint\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Decreased Validation Loss ({} ==> {})   < model saved >'.format(valid_loss_min,valid_loss))\n",
    "        \n",
    "        checkpoint = {\n",
    "            'state_dict': model.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, './bestModel.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11254174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted F1: 0.8329938841330211\n"
     ]
    }
   ],
   "source": [
    "# Load best performance model\n",
    "checkpoint = torch.load('./bestModel.pt')\n",
    "trained_model = MyModel()\n",
    "\n",
    "\n",
    "\n",
    "trained_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# evaluate\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # load test dataset\n",
    "    test_x = torch.from_numpy(np.loadtxt('./HW4_testX.csv',delimiter=',',dtype=np.float32))\n",
    "    test_y = torch.from_numpy(np.loadtxt('./HW4_testY.csv',delimiter=',',dtype=np.float32).reshape(-1,1))\n",
    "    \n",
    "    # evaluate using weighted f1 score.\n",
    "    y_pred = trained_model(test_x)\n",
    "    yp = y_pred.detach().numpy()\n",
    "    yp = [1.0 if x > 0.5 else 0.0 for x in yp]\n",
    "\n",
    "    print(\"weighted F1:\", f1_score(test_y, yp, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d260d678",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f3c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dfe19011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.228919</td>\n",
       "      <td>6.463900</td>\n",
       "      <td>0.472254</td>\n",
       "      <td>8.241039</td>\n",
       "      <td>-7.986439</td>\n",
       "      <td>4.419828</td>\n",
       "      <td>7.937655</td>\n",
       "      <td>-4.752053</td>\n",
       "      <td>-2.932872</td>\n",
       "      <td>-3.783504</td>\n",
       "      <td>...</td>\n",
       "      <td>1.103139</td>\n",
       "      <td>6.542378</td>\n",
       "      <td>5.916002</td>\n",
       "      <td>0.869227</td>\n",
       "      <td>-0.654664</td>\n",
       "      <td>-3.400430</td>\n",
       "      <td>3.665243</td>\n",
       "      <td>-5.362047</td>\n",
       "      <td>0.960563</td>\n",
       "      <td>2.730822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.680726</td>\n",
       "      <td>-4.168741</td>\n",
       "      <td>7.034253</td>\n",
       "      <td>-5.463813</td>\n",
       "      <td>-0.575878</td>\n",
       "      <td>-4.839989</td>\n",
       "      <td>9.169139</td>\n",
       "      <td>1.040742</td>\n",
       "      <td>-7.860359</td>\n",
       "      <td>-0.310231</td>\n",
       "      <td>...</td>\n",
       "      <td>6.956896</td>\n",
       "      <td>-7.645589</td>\n",
       "      <td>-5.353952</td>\n",
       "      <td>-1.718410</td>\n",
       "      <td>2.977554</td>\n",
       "      <td>1.760390</td>\n",
       "      <td>6.545843</td>\n",
       "      <td>-5.365576</td>\n",
       "      <td>6.347999</td>\n",
       "      <td>-10.805965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.887534</td>\n",
       "      <td>2.845743</td>\n",
       "      <td>-2.861815</td>\n",
       "      <td>-7.900907</td>\n",
       "      <td>2.281003</td>\n",
       "      <td>-3.394852</td>\n",
       "      <td>-1.058811</td>\n",
       "      <td>-4.785132</td>\n",
       "      <td>-1.281071</td>\n",
       "      <td>2.939634</td>\n",
       "      <td>...</td>\n",
       "      <td>4.970719</td>\n",
       "      <td>1.772561</td>\n",
       "      <td>-4.717122</td>\n",
       "      <td>0.332202</td>\n",
       "      <td>9.332535</td>\n",
       "      <td>-2.939443</td>\n",
       "      <td>-1.905870</td>\n",
       "      <td>-3.286218</td>\n",
       "      <td>3.845007</td>\n",
       "      <td>0.604373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.019826</td>\n",
       "      <td>-3.790430</td>\n",
       "      <td>-4.893445</td>\n",
       "      <td>-2.197804</td>\n",
       "      <td>-3.053076</td>\n",
       "      <td>1.294890</td>\n",
       "      <td>-5.630816</td>\n",
       "      <td>-7.947301</td>\n",
       "      <td>4.348945</td>\n",
       "      <td>10.246729</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.535092</td>\n",
       "      <td>0.822923</td>\n",
       "      <td>-2.708332</td>\n",
       "      <td>-6.332679</td>\n",
       "      <td>10.530745</td>\n",
       "      <td>-2.172284</td>\n",
       "      <td>-7.045495</td>\n",
       "      <td>8.866513</td>\n",
       "      <td>-2.950878</td>\n",
       "      <td>0.826856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.814451</td>\n",
       "      <td>-9.006926</td>\n",
       "      <td>-1.514610</td>\n",
       "      <td>-2.765532</td>\n",
       "      <td>6.146830</td>\n",
       "      <td>2.998480</td>\n",
       "      <td>9.140896</td>\n",
       "      <td>8.670193</td>\n",
       "      <td>8.397481</td>\n",
       "      <td>5.403070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420641</td>\n",
       "      <td>9.998662</td>\n",
       "      <td>-2.859708</td>\n",
       "      <td>3.442361</td>\n",
       "      <td>12.766651</td>\n",
       "      <td>3.086012</td>\n",
       "      <td>-5.230854</td>\n",
       "      <td>-4.750442</td>\n",
       "      <td>1.549153</td>\n",
       "      <td>-0.635022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>3.974874</td>\n",
       "      <td>-2.993205</td>\n",
       "      <td>-2.683005</td>\n",
       "      <td>1.079106</td>\n",
       "      <td>0.613689</td>\n",
       "      <td>0.035055</td>\n",
       "      <td>-4.698192</td>\n",
       "      <td>-3.738479</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>-6.480069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.499226</td>\n",
       "      <td>0.758832</td>\n",
       "      <td>-1.238975</td>\n",
       "      <td>4.747481</td>\n",
       "      <td>0.468105</td>\n",
       "      <td>4.014269</td>\n",
       "      <td>-2.281803</td>\n",
       "      <td>12.485242</td>\n",
       "      <td>3.654442</td>\n",
       "      <td>4.299384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>1.519937</td>\n",
       "      <td>-9.648294</td>\n",
       "      <td>-0.846710</td>\n",
       "      <td>5.303586</td>\n",
       "      <td>6.897678</td>\n",
       "      <td>-4.581512</td>\n",
       "      <td>-10.776951</td>\n",
       "      <td>3.953178</td>\n",
       "      <td>0.909904</td>\n",
       "      <td>-6.828432</td>\n",
       "      <td>...</td>\n",
       "      <td>1.933507</td>\n",
       "      <td>-2.223882</td>\n",
       "      <td>-5.727911</td>\n",
       "      <td>10.852921</td>\n",
       "      <td>2.892385</td>\n",
       "      <td>-0.214741</td>\n",
       "      <td>10.700461</td>\n",
       "      <td>-7.454326</td>\n",
       "      <td>-2.699429</td>\n",
       "      <td>4.263726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>0.866765</td>\n",
       "      <td>-0.082205</td>\n",
       "      <td>2.748877</td>\n",
       "      <td>-0.690968</td>\n",
       "      <td>4.544461</td>\n",
       "      <td>-2.363283</td>\n",
       "      <td>3.472220</td>\n",
       "      <td>2.743873</td>\n",
       "      <td>-4.181521</td>\n",
       "      <td>-8.709196</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.731414</td>\n",
       "      <td>-0.359130</td>\n",
       "      <td>9.178993</td>\n",
       "      <td>-15.711183</td>\n",
       "      <td>-3.151777</td>\n",
       "      <td>4.001902</td>\n",
       "      <td>-7.767521</td>\n",
       "      <td>4.687436</td>\n",
       "      <td>4.230262</td>\n",
       "      <td>1.366639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>1.117300</td>\n",
       "      <td>2.844754</td>\n",
       "      <td>-2.626271</td>\n",
       "      <td>-2.040471</td>\n",
       "      <td>-0.352758</td>\n",
       "      <td>8.903402</td>\n",
       "      <td>4.876278</td>\n",
       "      <td>-2.957271</td>\n",
       "      <td>-5.091562</td>\n",
       "      <td>0.800120</td>\n",
       "      <td>...</td>\n",
       "      <td>3.066929</td>\n",
       "      <td>3.055679</td>\n",
       "      <td>6.179786</td>\n",
       "      <td>-0.095293</td>\n",
       "      <td>7.500524</td>\n",
       "      <td>0.374477</td>\n",
       "      <td>6.998742</td>\n",
       "      <td>6.433831</td>\n",
       "      <td>-14.591685</td>\n",
       "      <td>1.387973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>-4.546920</td>\n",
       "      <td>0.790932</td>\n",
       "      <td>5.945616</td>\n",
       "      <td>2.823718</td>\n",
       "      <td>-5.713800</td>\n",
       "      <td>-6.330846</td>\n",
       "      <td>1.631730</td>\n",
       "      <td>7.926000</td>\n",
       "      <td>-8.581135</td>\n",
       "      <td>1.738259</td>\n",
       "      <td>...</td>\n",
       "      <td>3.176004</td>\n",
       "      <td>-3.481199</td>\n",
       "      <td>-5.221380</td>\n",
       "      <td>12.215304</td>\n",
       "      <td>-2.753013</td>\n",
       "      <td>2.502552</td>\n",
       "      <td>-4.324472</td>\n",
       "      <td>-9.878282</td>\n",
       "      <td>7.506859</td>\n",
       "      <td>6.785258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5          6   \\\n",
       "0      -7.228919  6.463900  0.472254  8.241039 -7.986439  4.419828   7.937655   \n",
       "1       0.680726 -4.168741  7.034253 -5.463813 -0.575878 -4.839989   9.169139   \n",
       "2       1.887534  2.845743 -2.861815 -7.900907  2.281003 -3.394852  -1.058811   \n",
       "3     -10.019826 -3.790430 -4.893445 -2.197804 -3.053076  1.294890  -5.630816   \n",
       "4      -1.814451 -9.006926 -1.514610 -2.765532  6.146830  2.998480   9.140896   \n",
       "...          ...       ...       ...       ...       ...       ...        ...   \n",
       "15995   3.974874 -2.993205 -2.683005  1.079106  0.613689  0.035055  -4.698192   \n",
       "15996   1.519937 -9.648294 -0.846710  5.303586  6.897678 -4.581512 -10.776951   \n",
       "15997   0.866765 -0.082205  2.748877 -0.690968  4.544461 -2.363283   3.472220   \n",
       "15998   1.117300  2.844754 -2.626271 -2.040471 -0.352758  8.903402   4.876278   \n",
       "15999  -4.546920  0.790932  5.945616  2.823718 -5.713800 -6.330846   1.631730   \n",
       "\n",
       "             7         8          9   ...         90        91        92  \\\n",
       "0     -4.752053 -2.932872  -3.783504  ...   1.103139  6.542378  5.916002   \n",
       "1      1.040742 -7.860359  -0.310231  ...   6.956896 -7.645589 -5.353952   \n",
       "2     -4.785132 -1.281071   2.939634  ...   4.970719  1.772561 -4.717122   \n",
       "3     -7.947301  4.348945  10.246729  ...  -2.535092  0.822923 -2.708332   \n",
       "4      8.670193  8.397481   5.403070  ...   0.420641  9.998662 -2.859708   \n",
       "...         ...       ...        ...  ...        ...       ...       ...   \n",
       "15995 -3.738479  0.801668  -6.480069  ...  -0.499226  0.758832 -1.238975   \n",
       "15996  3.953178  0.909904  -6.828432  ...   1.933507 -2.223882 -5.727911   \n",
       "15997  2.743873 -4.181521  -8.709196  ... -14.731414 -0.359130  9.178993   \n",
       "15998 -2.957271 -5.091562   0.800120  ...   3.066929  3.055679  6.179786   \n",
       "15999  7.926000 -8.581135   1.738259  ...   3.176004 -3.481199 -5.221380   \n",
       "\n",
       "              93         94        95         96         97         98  \\\n",
       "0       0.869227  -0.654664 -3.400430   3.665243  -5.362047   0.960563   \n",
       "1      -1.718410   2.977554  1.760390   6.545843  -5.365576   6.347999   \n",
       "2       0.332202   9.332535 -2.939443  -1.905870  -3.286218   3.845007   \n",
       "3      -6.332679  10.530745 -2.172284  -7.045495   8.866513  -2.950878   \n",
       "4       3.442361  12.766651  3.086012  -5.230854  -4.750442   1.549153   \n",
       "...          ...        ...       ...        ...        ...        ...   \n",
       "15995   4.747481   0.468105  4.014269  -2.281803  12.485242   3.654442   \n",
       "15996  10.852921   2.892385 -0.214741  10.700461  -7.454326  -2.699429   \n",
       "15997 -15.711183  -3.151777  4.001902  -7.767521   4.687436   4.230262   \n",
       "15998  -0.095293   7.500524  0.374477   6.998742   6.433831 -14.591685   \n",
       "15999  12.215304  -2.753013  2.502552  -4.324472  -9.878282   7.506859   \n",
       "\n",
       "              99  \n",
       "0       2.730822  \n",
       "1     -10.805965  \n",
       "2       0.604373  \n",
       "3       0.826856  \n",
       "4      -0.635022  \n",
       "...          ...  \n",
       "15995   4.299384  \n",
       "15996   4.263726  \n",
       "15997   1.366639  \n",
       "15998   1.387973  \n",
       "15999   6.785258  \n",
       "\n",
       "[16000 rows x 100 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523805c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
