{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1651157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset,ConcatDataset\n",
    "import numpy as np\n",
    "from pytorch_forecasting import MAPE\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Configure Random seeds\n",
    "torch.manual_seed(777)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(777)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08692620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('train_series.npy')\n",
    "trainset=[]\n",
    "train_x = train_data[:,:,0]\n",
    "train_y = train_data[:,:,1]\n",
    "\n",
    "print(type(train_x[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36cfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset - train set\n",
    "class HW5_trainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "\n",
    "        train_data = np.load('train_series.npy')\n",
    "        \n",
    "        train_x = train_data[:,:,0]\n",
    "        train_y = train_data[:,:,1]\n",
    "        \n",
    "        \n",
    "        self.x_data=torch.from_numpy(train_x).float()\n",
    "        self.y_data=torch.from_numpy(train_y).float()\n",
    "\n",
    "        self.len= len(train_x)\n",
    " \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Define custom dataset - test set    \n",
    "class HW5_testDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        test_data = np.load('test_series.npy')\n",
    "        \n",
    "        test_x = test_data[:,:,0]\n",
    "        test_y = test_data[:,:,1]\n",
    "        \n",
    "        self.len= test_x.shape[0]\n",
    "        self.x_data=torch.from_numpy(test_x).float()\n",
    "        self.y_data=torch.from_numpy(test_y).float()\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# Hyper parameter setting\n",
    "batch_size = 32\n",
    "\n",
    "# Create datsets and dataloader\n",
    "trainset = HW5_trainDataset()\n",
    "testset = HW5_testDataset()\n",
    "train_loader =DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
    "test_loader= DataLoader(testset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e77d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768816ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=5, num_layers=1, output_size=5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) \n",
    "        self.fc1 =  nn.Linear(hidden_size, 128) \n",
    "        self.fc2 = nn.Linear(128, output_size) \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "       \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        hn = hn.view(-1, self.hidden_size) \n",
    "        out = self.relu(hn)\n",
    "        out = self.fc1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc2(out) #Final Output\n",
    "        return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990333f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 1653 elements\n"
     ]
    }
   ],
   "source": [
    "model = LSTM()\n",
    "loss_function = MAPE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "\n",
    "params = list(model.parameters())\n",
    "print(\"The number of parameters:\", sum([p.numel() for p in model.parameters() if p.requires_grad]), \"elements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e8ed03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(5, 5, batch_first=True)\n",
      "  (fc_1): Linear(in_features=5, out_features=128, bias=True)\n",
      "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb11d1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Training Loss: 1.0290913022756576    Validation Loss: 0.9625849843025207\n",
      "Decreased Validation Loss (1000000000 ==> 0.9625849843025207)   < model saved >\n",
      "Epoch: 1     Training Loss: 0.8734748996496201    Validation Loss: 0.7947734022140502\n",
      "Decreased Validation Loss (0.9625849843025207 ==> 0.7947734022140502)   < model saved >\n",
      "Epoch: 2     Training Loss: 0.7773243390321731    Validation Loss: 0.692499788403511\n",
      "Decreased Validation Loss (0.7947734022140502 ==> 0.692499788403511)   < model saved >\n",
      "Epoch: 3     Training Loss: 0.7297347221970558    Validation Loss: 0.663724578499794\n",
      "Decreased Validation Loss (0.692499788403511 ==> 0.663724578499794)   < model saved >\n",
      "Epoch: 4     Training Loss: 0.7049571027755738    Validation Loss: 0.6127844413518906\n",
      "Decreased Validation Loss (0.663724578499794 ==> 0.6127844413518906)   < model saved >\n",
      "Epoch: 5     Training Loss: 0.6908256575465203    Validation Loss: 0.575508530497551\n",
      "Decreased Validation Loss (0.6127844413518906 ==> 0.575508530497551)   < model saved >\n",
      "Epoch: 6     Training Loss: 0.6701189658641815    Validation Loss: 0.5641239771842956\n",
      "Decreased Validation Loss (0.575508530497551 ==> 0.5641239771842956)   < model saved >\n",
      "Epoch: 7     Training Loss: 0.6586882402896881    Validation Loss: 0.559128603875637\n",
      "Decreased Validation Loss (0.5641239771842956 ==> 0.559128603875637)   < model saved >\n",
      "Epoch: 8     Training Loss: 0.6646082178354263    Validation Loss: 0.548624639749527\n",
      "Decreased Validation Loss (0.559128603875637 ==> 0.548624639749527)   < model saved >\n",
      "Epoch: 9     Training Loss: 0.6423275126814842    Validation Loss: 0.5352819693088532\n",
      "Decreased Validation Loss (0.548624639749527 ==> 0.5352819693088532)   < model saved >\n",
      "Epoch: 10     Training Loss: 0.6400816079974174    Validation Loss: 0.5365124363303184\n",
      "Epoch: 11     Training Loss: 0.637879977107048    Validation Loss: 0.5272202271819114\n",
      "Decreased Validation Loss (0.5352819693088532 ==> 0.5272202271819114)   < model saved >\n",
      "Epoch: 12     Training Loss: 0.6285493225455284    Validation Loss: 0.5443909511566162\n",
      "Epoch: 13     Training Loss: 0.6287010592818261    Validation Loss: 0.5158485232591629\n",
      "Decreased Validation Loss (0.5272202271819114 ==> 0.5158485232591629)   < model saved >\n",
      "Epoch: 14     Training Loss: 0.6303445687890052    Validation Loss: 0.5018625753521919\n",
      "Decreased Validation Loss (0.5158485232591629 ==> 0.5018625753521919)   < model saved >\n",
      "Epoch: 15     Training Loss: 0.6214305454492569    Validation Loss: 0.5317398415803909\n",
      "Epoch: 16     Training Loss: 0.6298426724076271    Validation Loss: 0.524036019563675\n",
      "Epoch: 17     Training Loss: 0.6186908538341522    Validation Loss: 0.5046765003800392\n",
      "Epoch: 18     Training Loss: 0.6257730849385261    Validation Loss: 0.5252879024147987\n",
      "Epoch: 19     Training Loss: 0.621042325079441    Validation Loss: 0.5071822969317437\n",
      "Epoch: 20     Training Loss: 0.6168159200549126    Validation Loss: 0.49485877114534377\n",
      "Decreased Validation Loss (0.5018625753521919 ==> 0.49485877114534377)   < model saved >\n",
      "Epoch: 21     Training Loss: 0.6116368407011032    Validation Loss: 0.5147304621934891\n",
      "Epoch: 22     Training Loss: 0.6138820995092392    Validation Loss: 0.4936805277466774\n",
      "Decreased Validation Loss (0.49485877114534377 ==> 0.4936805277466774)   < model saved >\n",
      "Epoch: 23     Training Loss: 0.6078089842200279    Validation Loss: 0.4966111794114113\n",
      "Epoch: 24     Training Loss: 0.6092787737250328    Validation Loss: 0.5046123247146607\n",
      "Epoch: 25     Training Loss: 0.6052785738110542    Validation Loss: 0.5038780695199967\n",
      "Epoch: 26     Training Loss: 0.6028269364833831    Validation Loss: 0.4921361123919487\n",
      "Decreased Validation Loss (0.4936805277466774 ==> 0.4921361123919487)   < model saved >\n",
      "Epoch: 27     Training Loss: 0.5988073454499244    Validation Loss: 0.4854351963400841\n",
      "Decreased Validation Loss (0.4921361123919487 ==> 0.4854351963400841)   < model saved >\n",
      "Epoch: 28     Training Loss: 0.6002396527528763    Validation Loss: 0.4959101127386093\n",
      "Epoch: 29     Training Loss: 0.6006571217179298    Validation Loss: 0.4810084026455879\n",
      "Decreased Validation Loss (0.4854351963400841 ==> 0.4810084026455879)   < model saved >\n",
      "Epoch: 30     Training Loss: 0.5926737310290336    Validation Loss: 0.503642753303051\n",
      "Epoch: 31     Training Loss: 0.5945945712327957    Validation Loss: 0.4829971408247948\n",
      "Epoch: 32     Training Loss: 0.5941504984498024    Validation Loss: 0.5099290992021561\n",
      "Epoch: 33     Training Loss: 0.5913124583959579    Validation Loss: 0.4845109419822693\n",
      "Epoch: 34     Training Loss: 0.5928622255921364    Validation Loss: 0.49295204132795334\n",
      "Epoch: 35     Training Loss: 0.5890580240488053    Validation Loss: 0.49473202401399613\n",
      "Epoch: 36     Training Loss: 0.5928930593729019    Validation Loss: 0.48470249873399734\n",
      "Epoch: 37     Training Loss: 0.5903975152969361    Validation Loss: 0.48893728679418563\n",
      "Epoch: 38     Training Loss: 0.5856453656554222    Validation Loss: 0.49535918527841566\n",
      "Epoch: 39     Training Loss: 0.5914460876584053    Validation Loss: 0.4815624037384987\n",
      "Epoch: 40     Training Loss: 0.5879109631180763    Validation Loss: 0.4755214838385582\n",
      "Decreased Validation Loss (0.4810084026455879 ==> 0.4755214838385582)   < model saved >\n",
      "Epoch: 41     Training Loss: 0.5854307479858398    Validation Loss: 0.4889599095582962\n",
      "Epoch: 42     Training Loss: 0.5859164589643479    Validation Loss: 0.4866142233014107\n",
      "Epoch: 43     Training Loss: 0.5847660961151123    Validation Loss: 0.47204235529899596\n",
      "Decreased Validation Loss (0.4755214838385582 ==> 0.47204235529899596)   < model saved >\n",
      "Epoch: 44     Training Loss: 0.5820688624382019    Validation Loss: 0.4852032268643379\n",
      "Epoch: 45     Training Loss: 0.5842593242526054    Validation Loss: 0.4791575508117676\n",
      "Epoch: 46     Training Loss: 0.5822855497002601    Validation Loss: 0.47773172813653947\n",
      "Epoch: 47     Training Loss: 0.5810700386166573    Validation Loss: 0.4869296458363533\n",
      "Epoch: 48     Training Loss: 0.5805243551135063    Validation Loss: 0.48062749004364014\n",
      "Epoch: 49     Training Loss: 0.5808795872926712    Validation Loss: 0.4824886436462402\n",
      "Epoch: 50     Training Loss: 0.5797045533657074    Validation Loss: 0.49771885174512864\n",
      "Epoch: 51     Training Loss: 0.5797374943494796    Validation Loss: 0.4869340363740921\n",
      "Epoch: 52     Training Loss: 0.577873861849308    Validation Loss: 0.47957142382860185\n",
      "Epoch: 53     Training Loss: 0.5792229819297791    Validation Loss: 0.48272347074747085\n",
      "Epoch: 54     Training Loss: 0.5775270463228226    Validation Loss: 0.48176639157533646\n",
      "Epoch: 55     Training Loss: 0.5786334878206253    Validation Loss: 0.47832168531417846\n",
      "Epoch: 56     Training Loss: 0.5765941492319107    Validation Loss: 0.4864449988603592\n",
      "Epoch: 57     Training Loss: 0.5773987923264503    Validation Loss: 0.47962796670198443\n",
      "Epoch: 58     Training Loss: 0.5753120381236076    Validation Loss: 0.5039258624315262\n",
      "Epoch: 59     Training Loss: 0.5748817991614342    Validation Loss: 0.4754875518083572\n",
      "Epoch: 60     Training Loss: 0.5742874377965927    Validation Loss: 0.4784445480108261\n",
      "Epoch: 61     Training Loss: 0.5740135521888733    Validation Loss: 0.5101285597681999\n",
      "Epoch: 62     Training Loss: 0.5757785122394562    Validation Loss: 0.48761180382966995\n",
      "Epoch: 63     Training Loss: 0.5738258343935013    Validation Loss: 0.4739112832546234\n",
      "Epoch: 64     Training Loss: 0.573171271264553    Validation Loss: 0.4903126701116562\n",
      "Epoch: 65     Training Loss: 0.5733404458761215    Validation Loss: 0.4768331449627876\n",
      "Epoch: 66     Training Loss: 0.5717749458551407    Validation Loss: 0.47599650752544403\n",
      "Epoch: 67     Training Loss: 0.5750480483174324    Validation Loss: 0.47628192019462584\n",
      "Epoch: 68     Training Loss: 0.57514165610075    Validation Loss: 0.4748356317281723\n",
      "Epoch: 69     Training Loss: 0.5746075546145439    Validation Loss: 0.4775524687170982\n",
      "Epoch: 70     Training Loss: 0.5721001489162445    Validation Loss: 0.4910360652208328\n",
      "Epoch: 71     Training Loss: 0.575568571805954    Validation Loss: 0.4851282980442047\n",
      "Epoch: 72     Training Loss: 0.573177499294281    Validation Loss: 0.4845903932452202\n",
      "Epoch: 73     Training Loss: 0.5724360817074776    Validation Loss: 0.4815622411370277\n",
      "Epoch: 74     Training Loss: 0.5727026044726372    Validation Loss: 0.5032367304563522\n",
      "Epoch: 75     Training Loss: 0.5711904571056367    Validation Loss: 0.48206888580322266\n",
      "Epoch: 76     Training Loss: 0.5728574007153511    Validation Loss: 0.4865456762313843\n",
      "Epoch: 77     Training Loss: 0.5714623818397522    Validation Loss: 0.5013272415399551\n",
      "Epoch: 78     Training Loss: 0.5732369376420975    Validation Loss: 0.47990878033638\n",
      "Epoch: 79     Training Loss: 0.5726753161549568    Validation Loss: 0.47756287950277326\n",
      "Epoch: 80     Training Loss: 0.573623097717762    Validation Loss: 0.48683072060346605\n",
      "Epoch: 81     Training Loss: 0.5722758942246438    Validation Loss: 0.473375614464283\n",
      "Epoch: 82     Training Loss: 0.5714473575949669    Validation Loss: 0.4804196565151215\n",
      "Epoch: 83     Training Loss: 0.5724617930054665    Validation Loss: 0.49560163456201556\n",
      "Epoch: 84     Training Loss: 0.5716995444297791    Validation Loss: 0.47588138312101363\n",
      "Epoch: 85     Training Loss: 0.5726210671067238    Validation Loss: 0.4847922203540802\n",
      "Epoch: 86     Training Loss: 0.5704642330408096    Validation Loss: 0.49144940841197965\n",
      "Epoch: 87     Training Loss: 0.5696142753958702    Validation Loss: 0.49250297570228574\n",
      "Epoch: 88     Training Loss: 0.5717633716464042    Validation Loss: 0.4831798421740532\n",
      "Epoch: 89     Training Loss: 0.5707240181565285    Validation Loss: 0.47366529059410095\n",
      "Epoch: 90     Training Loss: 0.5712849251031875    Validation Loss: 0.4874122282266617\n",
      "Epoch: 91     Training Loss: 0.5705614633560181    Validation Loss: 0.4978457179069519\n",
      "Epoch: 92     Training Loss: 0.5698925008773804    Validation Loss: 0.49030469381809233\n",
      "Epoch: 93     Training Loss: 0.5707565567493439    Validation Loss: 0.47505665028095245\n",
      "Epoch: 94     Training Loss: 0.5710529184341431    Validation Loss: 0.46398472023010257\n",
      "Decreased Validation Loss (0.47204235529899596 ==> 0.46398472023010257)   < model saved >\n",
      "Epoch: 95     Training Loss: 0.5704393788576126    Validation Loss: 0.48446101552248\n",
      "Epoch: 96     Training Loss: 0.5706739186644554    Validation Loss: 0.47892807447910307\n",
      "Epoch: 97     Training Loss: 0.5702429233193398    Validation Loss: 0.495089380979538\n",
      "Epoch: 98     Training Loss: 0.568746045589447    Validation Loss: 0.49003636139631274\n",
      "Epoch: 99     Training Loss: 0.571069530904293    Validation Loss: 0.49416046476364134\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "# train the model\n",
    "valid_loss_min = int(1e9)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss=0.0\n",
    "    valid_loss=0.0\n",
    "    \n",
    "    # Train \n",
    "    \n",
    "    model.train()\n",
    "    # loop for ever batch set in train dataset loader\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data = data.reshape(-1,1,5)\n",
    "        # initialize optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_size),\n",
    "                        torch.zeros(1, 1, model.hidden_size))\n",
    "        \n",
    "        # train\n",
    "        y_pred = model(data)\n",
    "        \n",
    "        # caculate batch loss\n",
    "        loss = loss_function(y_pred, target)\n",
    "        \n",
    "        # update\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Record train loss\n",
    "        train_loss += loss.item() \n",
    "    \n",
    "    # Validate \n",
    "    \n",
    "    model.eval()\n",
    "    # loop for ever batch set in test dataset loader\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data = data.reshape(-1,1,5)\n",
    "        y_pred = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = loss_function(y_pred, target)\n",
    "        \n",
    "        # calculate validataion loss\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    valid_loss = valid_loss / len(test_loader)\n",
    "    \n",
    "    # print training/validation results\n",
    "    print('Epoch: {}     Training Loss: {}    Validation Loss: {}'.format(epoch,train_loss, valid_loss))\n",
    "    \n",
    "    # if trained model perform best vaildation loss, then save it to the checkpoint\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Decreased Validation Loss ({} ==> {})   < model saved >'.format(valid_loss_min,valid_loss))\n",
    "        \n",
    "        checkpoint = {\n",
    "            'state_dict': model.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, './bestModel.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "09b5762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in terms of the mean absolute percentage error (MAPE): 0.4647793471813202\n"
     ]
    }
   ],
   "source": [
    "# Load best performance model\n",
    "checkpoint = torch.load('./bestModel.pt')\n",
    "trained_model = LSTM()\n",
    "\n",
    "trained_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# evaluate\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_data = np.load('test_series.npy')\n",
    "    # load test dataset\n",
    "    test_x = test_data[:,:,0]\n",
    "    test_y = test_data[:,:,1]\n",
    "\n",
    "    \n",
    "    test_x=torch.from_numpy(test_x).float()\n",
    "    test_y=torch.from_numpy(test_y).float()\n",
    "    test_x = test_x.reshape(-1,1,5)\n",
    "\n",
    "    y_pred = trained_model(test_x)\n",
    "    loss = loss_function(y_pred, test_y)\n",
    "    print(\"accuracy in terms of the mean absolute percentage error (MAPE):\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885853b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
